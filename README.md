# master degree project: a forensic grographic corrdinate prediction pipeline
#### course: degree project
#### credit: 45
#### student: Yuexin Yu
#### Supervisor: Eran Elhaik

This is the README file containing the workflow to construct this forensic geopragic cooridnate prediction pipeline.

The workflow randomly split AADR set into 2 subsets, select AIMs based on these 2 subsets, and predict the distance from the origin by the ancestral population portion calculated, based on selected AIMs, for 100 times. The selected AIM set from the best set, having the smallest predicted distance from the origin, becomes the AIM set used for any new test set. The performance of selected AIM sets are measured by the median of distance from the origin prediction inï¼š

* using the selected AIM set from one subset in the best split set to predict the other subset;

* using either selected AIM set from one of subset in the best split set to predict a new test set (output_645);

* using leave-one-out precedure to predict the whole AADR set.

Due to the size limitation of github, not all required input are available on github. Some of files are in *Data* directory, and some of files are in the main directory for the consistency to the following code

All required packages in R are listed in packages.R  

> Load packages in R 4.1.2
``` r
library(caret)
library(dplyr)
library(plyr)
library(geosphere)
library(maps)
library(ggplot2)
library(lightgbm)
library(geosphere)
library(xgboost)
library(e1071)
library(randomForest)
library(openxlsx)
library(countrycode)
library(MazamaSpatialUtils)
library(scatterpie)
```

  
> 0. Functions
```r
rf_latlong <- function(training, testing, variables, coast=NULL, nthread=8){
  # Model training by random forest method and use the model to predict test set. Then adjust predicted geographic coordinates based on world point data and coastline data
  # @training: the data frame of training set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @test: the data frame of test set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @variables: a list of column names used in model training
  # @coast: If coast = NULL, which is the feault, it means no need data adjustment after prediction; otherwise, it should be the coastline data generated in rf_model_training function. 
  # @nthread: number of threads to use when running this function. The default is 8.
  
  set.seed(1234)
  
  # create 5 subsets in training for cross validation
  folds <- createFolds(training[,'Populations'], k = 5, returnTrain = T)
  
  # ask cross validation when model training; the model training will be a regression
  trControl <-  trainControl( # regression
    method = "cv",
    number = 5,
    verboseIter = FALSE,
    returnData = FALSE,
    search = "grid",
    savePredictions = "final",
    allowParallel = T,
    index = folds)
  
  # use a range of value as the value of parameter so that all value will be tested and the model with smallest RMSE will be finally used
  tune_grid <- expand.grid(.mtry = c(1:15))
  
  # train for latitude
  training$rowIndex <- as.numeric(rownames(training))
  Xgb_latitude <- train(x = training[,variables],
                        y = training[,'latitude'],
                        method = "rf",
                        trControl = trControl,
                        tuneGrid = tune_grid ,
                        nthread = nthread
  )
  
  # train for longitude
  Xgb_longitude <- train(x = training[,variables],
                         y = training[,'longitude'],
                         method = "rf",
                         trControl = trControl,
                         tuneGrid = tune_grid ,
                         nthread = nthread
  )
  
  # predict the test set
  latPred <- predict(Xgb_latitude, newdata = testing[,variables])
  longPred <- predict(Xgb_longitude, newdata = testing[,variables])
  
  #adjust out of bounds predictions
  message('adjust out of bounds predictions')
  longPred[longPred > 180] <- 180
  longPred[longPred < -180] <- -180
  latPred[latPred > 90] <- 90
  latPred[latPred < -90] <- -90
  #Pull to nearest coastline if provided
  message('Pull to nearest coastline if provided')
  find_coast <- function(long, lat) { # find the closet point on the coast for the given long and lat
    distances_from_coastline <- sp::spDistsN1(coast, c(long, lat), longlat = TRUE)
    
    closest_point <-  which.min(distances_from_coastline)
    new_coords <- coast[closest_point,]
    
    return(new_coords)
    
  }
  
  if (!is.null(coast)) {
    message('toAdjust generated by function map.where')
    # find the longPred / latPred that are not on world latitude/longitude
    toAdjust <-
      which(is.na(maps::map.where(database = "world", longPred, latPred)))
    
    if(length(toAdjust) > 0){
      # apply find_coast function to adjust the latitude and longitude of given toAdjust index
      message('adjusted generated by mapply find_coast and longPred, latPred')
      adjusted <- mapply(find_coast, long = longPred[toAdjust], lat = latPred[toAdjust])
      
      # update the adjusted lat and long
      longPred[toAdjust] <- adjusted[1,]
      latPred[toAdjust] <- adjusted[2,]
    }
  }
  
  mean_r2 <- mean(c(Xgb_longitude$results$Rsquared[which(Xgb_longitude$results$mtry == Xgb_longitude$bestTune[[1]])],
                    Xgb_latitude$results$Rsquared[which(Xgb_latitude$results$mtry == Xgb_latitude$bestTune[[1]])]
  ))
  
  message('return')
  return(list(list(latPred, longPred),
              mean_r2) )
  
  
}


rf_model_training <- function(qfile_nogp_popFilter, tag, extraColumn=''){
  
  # @qfile_nogp_popFilter: the data frame with sample names, corresponding ancestral population portion calculations, sample's latitude and longitude, GRC, as well as country
  # @tag: a string which is the suffix added in the file path 
  # @extraColumn: a string. If run model training in random forest method, extraColumn = '', which is the default. If run model training in random forest with an extra column added, extraColumn = <the extra column name>
  
  # prepare coastline data for the adjustment of predicted geographic coordinate
  coastlines <- cbind("x"  = maps::SpatialLines2map(rworldmap::coastsCoarse)$x ,"y" =maps::SpatialLines2map(rworldmap::coastsCoarse)$y)
  coastlines <- coastlines[complete.cases(coastlines),]
  coastlines <- coastlines[coastlines[,1] < 180 ,]
  
  GeoPreds <- list()
  
  # extract the ancestral population names
  gp <- colnames(qfile_nogp_popFilter)[-c(1,2,12,13,14)]
  
  # add the name of extra column if needed
  if(extraColumn != ''){
    gp <- c(gp, extraColumn)
    
  }
  
  set.seed(18)
  
  # split qfile_nogp_popFilter into 5 subsets.
  trainFolds <-  caret::createFolds(qfile_nogp_popFilter$Populations, k = 5, returnTrain = T)
  
  GeoPreds <- list()
  
  # For each iteration, use 4 subsets as training set, the left one as test set to do model training and prediction. Since there are 5 subsets, iteration is 5 so that all samples are finally tested
  start_time <- Sys.time()
  for (i in 1:5){
    
    train <- qfile_nogp_popFilter[trainFolds[[i]],]
    test <- qfile_nogp_popFilter[-trainFolds[[i]],]
    
    start_time.adm <- Sys.time()
    testPreds <-rf_latlong(training = train, testing = test,
                           variables = gp, coast=coastlines)
    GeoPreds[[i]] <- testPreds
    
    end_time.adm <- Sys.time()
    time_for_testPreds <- end_time.adm - start_time.adm
    # message(time_for_testPreds)
    
    
  }
  end_time <- Sys.time()
  time_for_testPreds_ind <- end_time - start_time
  message(time_for_testPreds_ind) # 
  
  # Add predicted geographic coordinates to corresponding samples
  add_preds <- list()
  for (i in 1:5){
    
    add_preds[[i]] <- cbind(qfile_nogp_popFilter[-trainFolds[[i]],] ,
                            "latPred" = GeoPreds[[i]][[1]],
                            "longPred" = GeoPreds[[i]][[2]] )
    
  }
  
  # Add predicted geographic coordinates to corresponding samples
  MetasubDataPreds <- plyr::rbind.fill(add_preds)
  
  # Save the data frame as a csv file
  if(extraColumn == ''){
    write.csv(MetasubDataPreds,paste0("qfile_predict_rf_",tag,".csv"))
    
  }else{
    write.csv(MetasubDataPreds,paste0("qfile_predict_extraCol_",tag,".csv"))
    
  }
  
  # MetasubDataPreds <- read.csv(paste0("qfile_predict_rf_",tag,".csv"))
  
  # Calculate the distance between the original and the predicted geographic coordinates, added to column 'Distance_from_origin'
  for (i in 1:nrow(MetasubDataPreds)){
    MetasubDataPreds[i,"Distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
  }
  
  # Print distance from origin results 
  print(c(mean(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin[which(MetasubDataPreds$Distance_from_origin < 100)])))
  
  if(!file.exists('../median_distance_from_the_origin')){
    system("touch median_distance_from_the_origin")
  }
  system(paste0("echo '",ii,",",median(MetasubDataPreds$Distance_from_origin ),"'  >> median_distance_from_the_origin"))

}


maf_tbl_generation <- function(marker_frq, marker_ped,
                               rdata_output_path){
  # Return a table having sample in row, SNP in column. Each cell is the minor allele frequency of the SNP in current column for the sample in current row.
  
  # @marker_frq: The data frame read from .frq file
  # @marker_ped: The data frame read from .ped file
  # @rdata_output_path: A strin gof file path to save the MAF data frame in rdata
  
  marker_frq <- marker_frq[-1,]
  minor_allele.df <- data.frame(SNP = marker_frq$V2, MA = marker_frq$V3)
  
  marker_ped.allele <-  marker_ped[,c(1,7:ncol(marker_ped))] # row: individual column: genotype (biallelic)
  
  print('Start MAF')
  
  # Minor allele frequencies calculation
  start_time <- Sys.time()
  pop <- unique(marker_ped.allele$V1)
  if(length(pop) > 1){
    marker_freq <- data.frame(population = pop) # note that iteration of marker_ped.allele$V1 = marker_freq$individual
    for(i in 1:nrow(minor_allele.df)){ # i is index of marker
      if(i %% 10000 == 0){ # easy to track
        message(paste('Currently in i=', i))
      }
      freqs <- c()
      minor_allele <- minor_allele.df[i,2]
      for(j in pop){ #j is individual
        pop_ave <- mean(stringr::str_count(marker_ped.allele[which(marker_ped.allele$V1 %in% j),1+i],
                                           pattern = as.character(minor_allele))
                        * 0.5 ) # get the average of minor allele frequency of one population for current marker
        freqs <- c(freqs, pop_ave)
      }
      marker_freq <- cbind(marker_freq, freqs) # bind the minor allele frequencies for all individual in this marker to the result data frame
    }
  }else{# length(pop) = 1
    marker_freq <- data.frame(population = marker_ped.allele$V1) # note that iteration of marker_ped.allele$V1 = marker_freq$individual
    for(i in 1:nrow(minor_allele.df)){ # i is index of marker
      if(i %% 10000 == 0){ # easy to track
        message(paste('Currently in i=', i))
      }
      freqs <- c()
      minor_allele <- minor_allele.df[i,2]
      for(j in 1:nrow(marker_ped.allele)){ #j is individual
        # print(paste('j: ', j))
        pop_ave <- mean(stringr::str_count(marker_ped.allele[j,1+i],
                                           pattern = as.character(minor_allele))
                        * 0.5 ) # get the average of minor allele frequency of one population for current marker
        freqs <- c(freqs, pop_ave)
        
      }
      marker_freq <- cbind(marker_freq, freqs) # bind the minor allele frequencies for current individual in this marker to the result data frame
      
    }
  }
  
  colnames(marker_freq) <- c('Populations', minor_allele.df$SNP) # SNP name as column name
  
  end_time <- Sys.time()
  time_for_marker_freq <- end_time - start_time
  print(time_for_marker_freq)
  
  # Save MAF table
  metasub_data<- marker_freq
  save(metasub_data, file = rdata_output_path)
  return(metasub_data)
}


add_meta_ref <- function(metasub_data, meta){
  # Return metasub_data with corresponding country, latitude, longitude added for all samples
  # This function is only used for output_645 set

  # @metasub_data: A data frame having population and GRC information for all samples from output_645 set
  # @meta: A data frame having meta data for output_645 set
  
  # Remove meta data not having enough information
  meta_latlong <- meta[which(sapply(X = meta$inCountry, FUN = isTRUE)),]
  
  # Remove samples having population name that not in the meta data
  metasub_data <- metasub_data[which(metasub_data$Populations %in% meta_latlong$nameArgument),]
  meta_latlong$country <- make.names(meta_latlong$country)
  
  # Add country, latitude, longitude to the corresponding population
  metasub_data$country <- NA
  metasub_data$latitude <- NA
  metasub_data$longitude <- NA
  for(i in 1:nrow(metasub_data)){
    the_population <- metasub_data$Populations[i]
    metasub_data$country[i] <- meta_latlong$country[which(meta_latlong$nameArgument == the_population)]
    metasub_data$latitude[i] <- meta_latlong$latitidue[which(meta_latlong$nameArgument == the_population)]
    metasub_data$longitude[i] <- meta_latlong$longitude[which(meta_latlong$nameArgument == the_population)]
  }
  
  return(metasub_data)
}


add_meta_reich <- function(qfile_nogp, meta){
  # Return qfile_nogp with meta data (country, latitude, longitude) added, and population is also changed to corresponding version ID in AADR set
  # This function is only used for AADR set
  
  # @qfile_nogp: A data frame having population and GRC information for all samples from AADR set
  # @meta: A data frame having meta data for AADR set
  
  # filter out meta data not in given data frame, and the samples from the given data frame that not in meta data
  meta_pop <- meta[which(meta$Version.ID %in% qfile_nogp$GRC),]
  message(nrow(meta_pop))
  qfile_nogp_popFilter <- qfile_nogp[which(qfile_nogp$GRC %in% meta_pop$Version.ID),]
  message(nrow(qfile_nogp_popFilter)) # the number of rows in the given data frame and meta data should be the same after filtration
  
  meta_pop$Country <- make.names(meta_pop$Country)
  meta_pop$Version.ID <- as.character(meta_pop$Version.ID)
  
  # Add country, latitude, longitude to the corresponding population
  qfile_nogp_popFilter$country <- NA
  qfile_nogp_popFilter$latitude <- NA
  qfile_nogp_popFilter$longitude <- NA
  for(i in 1:nrow(qfile_nogp_popFilter)){
    versionID <- qfile_nogp_popFilter$GRC[i]
    qfile_nogp_popFilter$Populations[i] <- as.character(meta_pop$Group_Label[which(meta_pop$Version.ID == versionID)])
    qfile_nogp_popFilter$country[i] <- as.character(meta_pop$Country[which(meta_pop$Version.ID == versionID)])
    qfile_nogp_popFilter$latitude[i] <- as.numeric(as.character(meta_pop$Lat.[which(meta_pop$Version.ID == versionID)]))
    qfile_nogp_popFilter$longitude[i] <- as.numeric(as.character(meta_pop$Long.[which(meta_pop$Version.ID == versionID)]))
  }
  
  return(qfile_nogp_popFilter)
}


null_importance_select <-function(x, features, seed=123, shuffle=F,cores = 16) {
  # Return a feature importance table for all feature by fit model to latitude and longitude respectively, and sum the feature importance of both latitude and longitude as one feature's feature importance.
  
  # @x: A data frame containing columns used for model training, and the target to fit
  # @features: A list of feature names in x, which is used in model training
  # @shuffle: A boolean. If FALSE, the targets map to corresponding sample, which is the default; otherwise, the targets will be randomly shift to other samples
  # @cores: An integer. The number of cores to run this function
  
  
  y.lat <- x$latitude
  y.long <-x$longitude
  
  # Shuffle target if required
  if(isTRUE(shuffle)){
    
    new_order_x <- x[sample(nrow(x), size = nrow(x)),]
    y.lat <- new_order_x$latitude
    y.long <- new_order_x$longitude
  }
  
  dtrain.lat <- lgb.Dataset(data = as.matrix(x[,-c(ncol(x)-1,ncol(x))]), label = y.lat, free_raw_data=F)
  
  lgb_params <- list(objective = "regression",
                     boosting = 'rf',
                     subsample = 0.623,
                     colsample_bytree = 0.7,
                     num_leaves = 137,
                     max_depth = 8,
                     seed = seed,
                     bagging_freq = 1,
                     n_jobs = cores)
  
  # fit the model for latitude
  clf.lat <- lgb.train(params=lgb_params, data=dtrain.lat, nrounds=200, init_model=NULL)
  
  
  
  dtrain.long <- lgb.Dataset(data = as.matrix(x[,-c(ncol(x)-1,ncol(x))]), label = y.long, free_raw_data=F)
  
  # fit the model for longitude
  clf.long <- lgb.train(params=lgb_params, data=dtrain.long, nrounds=200)
  
  # Get feature importances, sum the feature importance in latitude and longitude as a feature's feature importance
  ftr_importance.long <- lgb.importance(clf.long, percentage = F)[,c('Feature','Gain', 'Frequency')]
  ftr_importance.lat <- lgb.importance(clf.lat, percentage = F)[,c('Feature','Gain', 'Frequency')]
  for(i in 1:nrow(ftr_importance.long)){
    current_fea <- ftr_importance.long$Feature[i]
    if(current_fea %in% ftr_importance.lat$Feature){ # lat + long
      ftr_importance.lat$Gain[which(ftr_importance.lat$Feature == current_fea)] <- sum(c(ftr_importance.lat$Gain[which(ftr_importance.lat$Feature == current_fea)], ftr_importance.long$Gain[i]))
      ftr_importance.lat$Frequency[which(ftr_importance.lat$Feature == current_fea)] <- sum(c(ftr_importance.lat$Frequency[which(ftr_importance.lat$Feature == current_fea)], ftr_importance.long$Frequency[i]))
    }else{
      ftr_importance.lat <- rbind(ftr_importance.lat, ftr_importance.long[i,])
    }
  }
  imp_df <- ftr_importance.lat
  colnames(imp_df) <- c('feature', 'importance_gain', 'importance_split')
  
  return(imp_df)
}


rf_model_training_train_test <- function(qfile_train_nogp_popFilter, qfile_test_nogp_popFilter, tag){
  # A pipeline to 
  # 1. run model training by random forest and prediction
  # 2. save predicted latitude and longitude, as well as mean R2 for trained model 
  
  # @qfile_train_nogp_popFilter: The data frame of training set, having sample names, corresponding ancestral population portion calculations, sample's latitude and longitude, GRC, as well as country 
  # @qfile_test_nogp_popFilter: The data frame of test set, having sample names, corresponding ancestral population portion calculations, sample's latitude and longitude, GRC, as well as country 
  # @tag: a list, 
  #       element 1: ind i.e. test_name
  #       element 2: ite i.e. selected_feature_set (baseline/bench/split300)
  
  if(ncol(qfile_train_nogp_popFilter) != ncol(qfile_test_nogp_popFilter)){
    stop(paste('num of columns in train and test is not equal for case', tag[[1]]))
  }
  
  # prepare coastline data for the adjustment of predicted geographic coordinate
  coastlines <- cbind("x"  = maps::SpatialLines2map(rworldmap::coastsCoarse)$x ,"y" =maps::SpatialLines2map(rworldmap::coastsCoarse)$y)
  coastlines <- coastlines[complete.cases(coastlines),]
  coastlines <- coastlines[coastlines[,1] < 180 ,]
  
  # extract columns used for model training
  gp <- colnames(qfile_train_nogp_popFilter)[-c(1,2,12,13,14)]
  
  # random forest model training and prediction
  start_time <- Sys.time()
  run_rlt <-rf_latlong(training = qfile_train_nogp_popFilter, 
                                  testing = qfile_test_nogp_popFilter,
                                  variables = gp, coast=coastlines)
  testPreds <- run_rlt[[1]]
  r2 <- run_rlt[[2]]
  
  end_time <- Sys.time()
  time_for_testPreds <- end_time- start_time
  message(time_for_testPreds)
  
  # write mean R2 value into a file
  system(paste0("echo '",tag[[1]],"_",tag[[2]],",",r2,"' >> mean_R2"))
  
  # add predicted results to the test set data frame
  add_preds <- cbind(qfile_test_nogp_popFilter,
                     "latPred" = testPreds[[1]],
                     "longPred" = testPreds[[2]] )
  
  # save test set data frame
  save(add_preds, file = paste0(tag[[1]],'_',tag[[2]],'_qfile.rdata'))
  MetasubDataPreds <- paste(add_preds, collapse = ',')
  
  system(paste0("echo '",MetasubDataPreds,"' >> ", tag[[2]], "_rf_rlt"))
  
}


mGPS <-function(training = NULL,
                testing = NULL,
                classTarget,
                hierarchy = c('genepool','population','latitude','longitude'),
                variables,
                nthread = 8,
                coast = NULL) {
  # Return predicted latitude and longitude using mGPS model
  
  # @training: the data frame of training set, having sample population, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @testing: the data frame of test set, having sample population, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @classTarget: a string of the last target to train model. The final model is trained by the training data frame, with predicted values from the models trained in the first 3 targets in hierarchy list hierarchically.  
  # @hierarchy: a list of ordered targets for model training hierarchically.
  # @variables: a list of column names used in model training
  # @nthread: number of threads to use when running this function. The default is 8.
  # @coast: If coast = NULL, which is the deault, it means no need data adjustment after prediction; otherwise, it should be the coastline data generated in rf_model_training function. 
  
  
  
  #Check for training set
  if(is.null(training)){
    return(message("No training set given"))
  } else{
    
    training <- droplevels(training)
    
    #Train mGPS with 5-fold cross validation of training set
    message("Training mGPS...")
    
    set.seed(1234)
    folds <- createFolds(training[,classTarget], k = 5, returnTrain = T)
    
    
    message('working on trainControl')
    trControlClass <-  trainControl(
      method = "cv",
      number = 5,  
      verboseIter = FALSE,
      returnData = FALSE,
      search = "grid",
      savePredictions = "final",
      classProbs = T,
      allowParallel = T,
      index = folds )
    
    
    message('working on trainControl without classProb')
    trControl <-  trainControl(
      method = "cv",
      number = 5,  
      verboseIter = FALSE,
      returnData = FALSE,
      search = "grid",
      savePredictions = "final",
      allowParallel = T,
      index = folds)
    
    
    tune_grid <- expand.grid(
      nrounds = c(300,600), 
      eta = c( 0.05, 0.1),
      max_depth = c(3,6,9),
      gamma = 0,
      colsample_bytree = c(0.6,0.8),
      min_child_weight = c(1),
      subsample = (0.7)
    )
    
    if(length(hierarchy) == 4){
      message('length(hierarchy)==4')
      message('model training')
      Xgb_region <- train(x = training[,variables],y = factor(training[,hierarchy[1]]),
                          method = "xgbTree",
                          trControl = trControlClass,
                          tuneGrid = tune_grid ,
                          nthread = nthread
      )
      
      l1_train <- data.frame(training[,c(variables)],Xgb_region[["pred"]][order(Xgb_region$pred$rowIndex),levels(factor(training[,hierarchy[1]])) ] )
      
    }else{
      message('hierachy != 4')
      message('model training')
      l1_train <- training[,c(variables)]
    }
    Xgb_class <- train(x = l1_train,y = factor(training[,classTarget]),
                       method = "xgbTree",
                       trControl = trControlClass,
                       tuneGrid = tune_grid,
                       nthread = nthread
    )
    
    l2_train <- data.frame(l1_train,Xgb_class[["pred"]][order(Xgb_class$pred$rowIndex),levels(factor(training[,classTarget])) ])
    
    Xgb_latitude <- train(x = l2_train ,y = training[,"latitude"],
                          method = "xgbTree",
                          trControl = trControl,
                          tuneGrid = tune_grid,
                          nthread = nthread
    )
    
    l3_train <- data.frame(l2_train, "latPred" = Xgb_latitude[["pred"]][order(Xgb_latitude$pred$rowIndex),"pred" ])
    
    Xgb_longitude <- train(x = l3_train ,y = training[,"longitude"],
                           method = "xgbTree",
                           trControl = trControl,
                           tuneGrid = tune_grid ,
                           nthread = nthread
    )
    
  }
  #check for test set, return trained model if no test set.
  if(is.null(testing)){
    
    model <- function(test,variables){
      regProbs <- predict(Xgb_region, newdata = test[,variables],type ="prob")
      
      l1_test <- data.frame(test[,variables], regProbs)
      
      classPred <- predict(Xgb_class, newdata = l1_test)
      classProbs <- predict(Xgb_class, newdata = l1_test,type ="prob")
      
      l2_test <-  data.frame(l1_test, classProbs) 
      latPred <- predict(Xgb_latitude, newdata = l2_test)
      
      l3_test <- data.frame(l2_test, latPred)
      longPred <- predict(Xgb_longitude, newdata = l3_test)
      return(list(classPred, latPred, longPred))
      
    }
    return(list(Xgb_region,Xgb_class,Xgb_latitude,Xgb_longitude,"model" = model))
  }else{
    message("Generating predictions")
    if(length(hierarchy) == 4){
      regProbs <- predict(Xgb_region, newdata = testing[,variables],type ="prob")
      
      l1_test <- data.frame(testing[,variables], regProbs)
    }else{
      l1_test <- testing[,variables]
    }
    classPred <- predict(Xgb_class, newdata = l1_test)
    classProbs <- predict(Xgb_class, newdata = l1_test,type ="prob")
    
    l2_test <-  data.frame(l1_test, classProbs) 
    latPred <- predict(Xgb_latitude, newdata = l2_test)
    
    l3_test <- data.frame(l2_test, latPred)
    longPred <- predict(Xgb_longitude, newdata = l3_test)
    
    #adjust out of bounds predictions
    message('adjust out of bounds predictions')
    longPred[longPred > 180] <- 180
    longPred[longPred < -180] <- -180
    latPred[latPred > 90] <- 90
    latPred[latPred < -90] <- -90
    #Pull to nearest coastline if provided
    message('Pull to nearest coastline if provided')
    find_coast <- function(long, lat) { # find the closet point on the coast for the given long and lat
      distances_from_coastline <-
        sp::spDistsN1(coast, c(long, lat), longlat = TRUE)
      
      closest_point <-  which.min(distances_from_coastline)
      new_coords <- coast[closest_point,]
      
      return(new_coords)
      
    }
    if (!is.null(coast)) {
      message('toAdjust generated by function map.where')
      # find the longPred / latPred that are not on world latitude/longitude
      toAdjust <-
        which(is.na(maps::map.where(database = "world", longPred, latPred)))
      
      # apply find_coast function to adjust the latitude and longitude of given toAdjust index 
      message('adjusted generated by mapply find_coast and longPred, latPred')
      adjusted <-
        mapply(find_coast, long = longPred[toAdjust], lat = latPred[toAdjust])
      
      # update the adjusted lat and long
      longPred[toAdjust] <- adjusted[1,]
      latPred[toAdjust] <- adjusted[2,]
      
      
    }
    
    
    message('return')
    return(list(classPred, latPred, longPred))
    
  }
  
}


barplot_admixture <- function(meta, pic_file_path){
  # Generate a PDF file having bar plot showing admixture portion for all sample, grouped by country, ordered by continent
  # The drawing part highly refers to the ADMIXTURE visualization tool constructed by Feng Q. (https://doi.org/10.1016/j.gpb.2018.05.002)
  
  # @meta: the data frame having population, admixture portion, latitude, longitude, country for all samples
  # @pic_file_path: a string of pdf output file path
  
  genepools <- c('NorthEastAsian', 'Mediterranean', 'SouthAfrican',
                 'SouthWestAsian', 'NativeAmerican', 'Oceanian',
                 'SouthEastAsian', 'NorthernEuropean', 'SubsaharanAfrican')
  
  barNaming <- function(col){
    retCol <- col
    for(i in 2:length(col)){
      if(stringr::str_trim(col[i-1]) == stringr::str_trim(col[i]) ){
        retCol[i] <- ''
      }
      
    }
    return(retCol)
  }
  
  # Convert lat and long to numeric
  meta$latitude <- sapply(meta$latitude, as.numeric)
  meta$longitude <- sapply(meta$longitude, as.numeric) 
  
  # Get the mean latitude and longitude for each country
  ave_ctry <- data.frame(country=NA, latitude=NA, longitude=NA)
  ctry_lst <- list()
  for(i in unique(meta$country)){
    ctry_lst[[i]] <- meta[which(meta$country == i),]
    
    ave_ctry <- rbind(ave_ctry, c(i, 
                                  mean(ctry_lst[[i]]$latitude), 
                                  mean(ctry_lst[[i]]$longitude)) )
    
  }
  ave_ctry <- ave_ctry[-1,]
  ave_ctry$latitude <- sapply(ave_ctry$latitude, as.numeric)
  ave_ctry$longitude <- sapply(ave_ctry$longitude, as.numeric) 
  str(ave_ctry)
  
  # Split countries to corresponding continent based on their mean lat and long
  africa_med <- data.frame()
  euro <- data.frame()
  asia <- data.frame()
  oceanian <- data.frame()
  america <- data.frame()
  for(j in 1:nrow(ave_ctry)){
    if((-34.9 < ave_ctry$latitude[j] & ave_ctry$latitude[j] < 43.8) & 
       (-28 < ave_ctry$longitude[j] & ave_ctry$longitude[j] < 58)){
      africa_med <- rbind(africa_med, ave_ctry[j,])
    }else if((36 < ave_ctry$latitude[j] & ave_ctry$latitude[j] < 72) & 
             (-28 < ave_ctry$longitude[j] & ave_ctry$longitude[j] < 59)){
      euro <- rbind(euro, ave_ctry[j,])
    }else if((-10 < ave_ctry$latitude[j] & ave_ctry$latitude[j] < 78) & 
             (59 < ave_ctry$longitude[j] & ave_ctry$longitude[j] < 180)){
      asia <- rbind(asia, ave_ctry[j,])
    }else if((-50 < ave_ctry$latitude[j] & ave_ctry$latitude[j] < -10) &
             (110 < ave_ctry$longitude[j] & ave_ctry$longitude[j] < 180)){
      oceanian <- rbind(oceanian, ave_ctry[j,])
    }else{
      america <- rbind(america, ave_ctry[j,])
    }
    
  }
  nrow(africa_med) + nrow(euro) + nrow(asia) + nrow(oceanian) + nrow(america) == length(unique(meta$country))
  
  # Sort countries based on:
  # descending for abs. lat
  # from left to right based on long
  africa_med <- africa_med %>% arrange(latitude, desc(longitude) )
  euro <- euro %>% arrange(latitude, desc(longitude))
  asia <- asia %>% arrange(longitude, desc(latitude))
  if(nrow(oceanian) != 0){
    oceanian <-  oceanian %>% arrange(desc(latitude), longitude)
  }
  america <- america %>% arrange(longitude, desc(latitude))
  
  add_inds_to_dt <- function(ctry_lst, continent){
    result_sub_dt <- data.frame()
    for(i in continent$country){
      result_sub_dt <- rbind(result_sub_dt, ctry_lst[[i]])
    }
    return(result_sub_dt)
  }
  tbl.ordered <- add_inds_to_dt(ctry_lst, africa_med)
  tbl.ordered <- rbind(tbl.ordered, add_inds_to_dt(ctry_lst, euro))
  tbl.ordered <- rbind(tbl.ordered, add_inds_to_dt(ctry_lst, asia))
  tbl.ordered <- rbind(tbl.ordered, add_inds_to_dt(ctry_lst, oceanian))
  tbl.ordered <- rbind(tbl.ordered, add_inds_to_dt(ctry_lst, america))
  nrow(tbl.ordered) == nrow(meta)
  
  if(nrow(tbl.ordered) >= 10000){
    # Add column for ind with country size < 20
    popsize <- as.data.frame(table(tbl.ordered[,12]))
    duplicate_line <- which(tbl.ordered$country %in% popsize$Var1[which(popsize$Freq < 20)])
    dup <- 19
    
    
  }else if(nrow(tbl.ordered) >= 1000){
    # Add column for ind with country size < 20
    popsize <- as.data.frame(table(tbl.ordered[,12]))
    duplicate_line <- which(tbl.ordered$country %in% popsize$Var1[which(popsize$Freq < 5)])
    # tbl.ordered_cp <- tbl.ordered
    # tbl.ordered <- tbl.ordered_cp
    dup <- 4
    
  }else{
    duplicate_line <- NA
  }
  
  if(length(duplicate_line)>1){
    ct <- 0
    
    for(i in duplicate_line){
      the_line <- as.data.frame(tbl.ordered[i+ct*dup,])
      tmp <- rbind(the_line, the_line[rep(1, dup),])
      half_up <-  tbl.ordered[c(1:(i+ct*dup-1)),]
      half_down <- tbl.ordered[c((i+ct*dup+1):nrow(tbl.ordered)),]
      if(i == 1){
        tbl.ordered <- rbind(tmp, half_down)
      }else if(i+ct*dup == nrow(tbl.ordered) ){
        tbl.ordered <- rbind(half_up, tmp)
        
      }else{
        tbl.ordered <- rbind(half_up, tmp, half_down)
      }
      ct <- ct+1
    }
    
  }
  
  sector <- function (x0=0, y0=0, angle1, angle2, radius1, radius2, col, angleinc = 0.03){
    if(isTRUE(angle1 > angle2)) {
      temp <- angle1
      angle1 <- angle2
      angle2 <- temp
    }
    if (isTRUE(radius1 > radius2)) {
      temp <- radius1
      radius1 <- radius2
      radius2 <- temp
    }
    
    # Use 4 points polygon to draw a sector
    angles <- seq(angle1, angle2, by = angleinc)
    angles[length(angles)] <- angle2
    angles <- angles*(pi/180)
    xpos <- c(cos(angles) * radius1, cos(rev(angles)) * radius2) + x0
    ypos <- c(sin(angles) * radius1, sin(rev(angles)) * radius2) + y0
    polygon(xpos, ypos, col = col, border = col)
  }
  
  colorPalette <- c('#984807', '#fcae91', '#632ea8',
                    '#cb181d', '#ffff00', '#ff00ff',
                    '#0000ff', '#00ff00', '#ff0000')
  
  # setting
  rmin <- 6
  rmax <- 1.85*rmin
  amin <- -251
  amax <- 91
  prgap <- 0.2
  col <- colorPalette
  indcol <- 1
  Kstart <- 3
  Knum <- 9
  pdf(pic_file_path, 45, 45)
  par(xpd=T,  mar=c(2, 2, 2, 2)) 
  plot(0, 0, xlim=c(-rmax, rmax), ylim=c(-rmax, rmax), axes=F, ann=F, type='n') # initialize the plot
  rstart <- rmin
  rlen <- (rmax-rmin)*(1-prgap) 
  data <- tbl.ordered
  angelperInd <- (amax - amin)/nrow(data) 
  angpre <- amin
  
  for ( i in 1:nrow(data) ){   # iteration for each individual
    ang1 <- angpre
    ang2 <- ang1 + angelperInd
    rpre <- rstart
    for ( j in Kstart:(Knum+Kstart-1) ){   # iteration for each K
      rpost <- rpre + rlen*data[i,j]
      sector(angle1=ang1, angle2=ang2, radius1=rpre, radius2=rpost, col=col[j-Kstart+1])   ###### draw sector for each K of each individual
      rpre <- rpost
    }
    angpre <- ang2
  }
  lend <- rmax - prgap*(rmax-rmin)
  
  #write all population name
  target <- unique(tbl.ordered$country)
  npop <- length(target)
  
  nameArgument <- barNaming(tbl.ordered$country)
  angelperpop <- (amax - amin)/nrow(data)
  angpre <- amin
  for ( i in 1:nrow(tbl.ordered) ){
    
    ang <- angpre + angelperpop
    xx <- lend*cos(ang*pi/180)
    yy <- lend*sin(ang*pi/180)
    text = nameArgument[i];
    cex_no = 220/length(nameArgument)
    if(cex_no>=0.1){cex_no=3}
    if(cex_no<0.1){cex_no=2}
    if ( ang < -90 ){text(xx, yy, text, srt=180+ang, adj=c(1.1, 0.5), cex=cex_no, font=1, col=colors()[490])}
    else {text(xx, yy, text, srt=ang, adj=c(-0.1, 0.5), cex=cex_no, font=1, col=colors()[490])
    }
    angpre <- angpre + angelperpop
  }
  dev.off()
  
}


distance_diff_plot_ctry <- function(MetasubDataPreds, png_path){
  # Generate bar plot for predicted distance from the origin in levels: <100, 100-500, 500-1000, 1000-2000, 2000-3000, >3000, in the domain of country
  
  # @MetasubDataPreds: the data frame having population, admixture portion, geographic coordinate, predicted geographic coordinate, country, distance from the origin for all samples
  # @png_path: a string of the plot path and name
  
  #Calculate distance from origin
  population_names <- unique(MetasubDataPreds$country)
  
  bar_df1 <- data.frame(row.names = c(population_names, "Overall"))

  # <100
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] < 100)
    bar_df1[rownames(bar_df1) == this_city,"0 - 100km"] <- prop
    if(prop > 0){print(this_city)
      print(prop)}
  }
  overall_prop <- mean(MetasubDataPreds[,"Distance_from_origin"] < 100)
  bar_df1[ nrow(bar_df1),"0 - 100km"] <- overall_prop
  
  # >= 100 & < 500
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] >= 100 & MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] < 500)
    bar_df1[rownames(bar_df1) == this_city,"100 - 500km"] <- prop
  }
  overall_prop <-mean(MetasubDataPreds[,"Distance_from_origin"] >= 100 & MetasubDataPreds[,"Distance_from_origin"] < 500)
  bar_df1[ nrow(bar_df1),"100 - 500km"] <- overall_prop
  
  # >= 500 & < 1000
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] >= 500 & MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] < 1000)
    bar_df1[rownames(bar_df1) == this_city,"500 - 1000km"] <- prop
  }
  overall_prop <- mean(MetasubDataPreds[,"Distance_from_origin"] >= 500 & MetasubDataPreds[,"Distance_from_origin"] < 1000)
  bar_df1[ nrow(bar_df1),"500 - 1000km"] <- overall_prop
  
  # >= 1000 & < 2000
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"]>= 1000 & MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] < 2000)
    bar_df1[rownames(bar_df1) == this_city,"1000 - 2000km"] <- prop
  }
  overall_prop <- mean(MetasubDataPreds[,"Distance_from_origin"] >= 1000 & MetasubDataPreds[,"Distance_from_origin"] < 2000)
  bar_df1[ nrow(bar_df1),"1000 - 2000km"] <- overall_prop
  
  # >= 2000 & < 3000
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] >= 2000 & MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] < 3000)
    bar_df1[rownames(bar_df1) == this_city,"2000 - 3000km"] <- prop
  }
  overall_prop <- mean(MetasubDataPreds[,"Distance_from_origin"] >= 2000 & MetasubDataPreds[,"Distance_from_origin"] < 3000)
  bar_df1[nrow(bar_df1),"2000 - 3000km"] <- overall_prop
  
  # >= 3000
  for (i in 1: length(population_names)){
    this_city <- population_names[i]
    prop <- mean(MetasubDataPreds[MetasubDataPreds$country == this_city,][,"Distance_from_origin"] > 3000 )
    bar_df1[rownames(bar_df1) == this_city,"> 3000km"] <- prop
  }
  overall_prop <- mean(MetasubDataPreds[,"Distance_from_origin"] > 3000)
  bar_df1[ nrow(bar_df1),"> 3000km"] <- overall_prop
  
  bar_df1$country <- rownames(bar_df1)
  

  population_counts <- as.data.frame(table(MetasubDataPreds$country))
  population_counts$Var1 <- as.character(population_counts$Var1)
  population_counts <- rbind(population_counts, c('Overall', 0))
  colnames(population_counts)[1] <- 'country'
  bar_df1 <- merge(x = bar_df1, y = population_counts[ , c("country", "Freq")], by = "country", all.x=TRUE)
  bar_df1 <- rbind(bar_df1[which(bar_df1$country == 'Overall'),], bar_df1[-which(bar_df1$country == 'Overall'),])

  bar_df1$title <- NA
  for(i in 2:nrow(bar_df1)){
    bar_df1$title[i] <- paste0(bar_df1$country[i],'(',bar_df1$Freq[i],')' )
  }

  png(png_path, width = 20, height = 8, units = 'in', res = 600)
  par(xpd = T, mar = par()$mar + c(10,1,0,2),  las=2, mgp = c(3,1,0))
  
  layout(mat = matrix(c(1, 2), 
                      nrow = 1, 
                      ncol = 2),
         widths = c(1, 8))     # Widths of the two columns
  
  bp1 <- barplot(t(bar_df1[1,c(2:7)]*100), space = 0,col=c("lightyellow","slategray1","lightblue", "skyblue", "royalblue3", "darkblue"), 
                names.arg=c("Overall",axes = FALSE) ,
                cex.names=.6, ylab = "", axisnames = F, axes = F) 
  axis(side =2, pos = 0)
  mtext(text = c("Overall"), 
  side = 1, at = bp1, line = 0, padj = 1, cex=0.7) 
  title(ylab="Proportion of sample predictions %", mgp=c(3,1,0),cex.lab=1, cex = 0.7)
  

  bp2 <- barplot(t(bar_df1[-1,c(2:7)]*100), space = 0,col=c("lightyellow","slategray1","lightblue", "skyblue", "royalblue3", "darkblue"), 
                names.arg=c(bar_df1$title[-1],
                             axes = FALSE) ,
                las =2, cex.names=.6, ylab = "", axisnames = F, axes = F) 
  axis(side =2, pos = 0)
  mtext(text = c(bar_df1$title[-1]), 
        side = 1, at = bp2, line = 0, padj = 1, cex=0.7) 

  legend("right",inset = c(-0.02,0), rev(c(colnames(bar_df1[,c(2:7)]))), 
         fill = rev(c("lightyellow","slategray1","lightblue", "skyblue", "royalblue3", "darkblue")) ,
         bty = 1, cex = 0.5)  
  
  

  dev.off()
  
  }


distance_map_visualization <- function(MetasubDataPreds, filter_level='all', png_path){
  # In this function:
  # 1. get the corresponding country and continent 
  # 2. determine if the predicted geographic coordinate is in the same ocontinent of the original geographic corrdinate 
  # 3. calculate the distance from the origin
  # 4. extract samples based on the filter_level
  # 5. calculate the portion of samples predicted to be in the same continent in continent domain, and assign the cooridnate to draw pie for each continent
  # 6. map visualization with pie chart
  
  # @MetasubDataPreds: a data frame containing origin latitud and longitude, origin country, origin continent, and predicted latitude and longitude columns
  # @filter_level: a string, 'all' or '500' or '1000'. 
  #               Default is 'all'. 
  #               'all' represents use all samples without filtration; 
  #               'notSameContinent500' means to remain samples having distance from the origin > 500km, and not in the same continent; 
  #               'notSameContinent1000' means to reamin samples having distance from the origin > 1000km, and not in the same continentï¼›
  #               'notSameContinent' means to reamin samples having distance from the origin > 1000km, and not in the same continent
  # @png_path: a string of the path to store the visualization
  
  ##### 1. get the corresponding country and continent  #####
  MetasubDataPreds$countryPred <- MazamaSpatialUtils::getCountry(MetasubDataPreds$longPred, MetasubDataPreds$latPred)
  MetasubDataPreds$continentPred <- countrycode(sourcevar = MetasubDataPreds$countryPred,
                                                origin = "country.name",
                                                destination = "continent")
  
  # manually change the incorrect country assignment
  for(i in 1:nrow(MetasubDataPreds)){
    if((MetasubDataPreds$longPred[i]< -33 & MetasubDataPreds$longPred[i] > -76) &
       (MetasubDataPreds$latPred[i]< 11 & MetasubDataPreds$latPred[i] > -56)){
      MetasubDataPreds$continentPred[i] <- 'Americas'
    }
  }
  
  ##### 2. determine if the predicted geographic coordinate is in the same ocontinent of the original geographic corrdinate  #####
  MetasubDataPreds$sameContinent <- NA
  for(i in 1:nrow(MetasubDataPreds)){
    if(MetasubDataPreds$continent[i] %in% MetasubDataPreds$continentPred[i]){
      MetasubDataPreds$sameContinent[i] <- TRUE
    }else{
      MetasubDataPreds$sameContinent[i] <- FALSE
    }
  }
  
  
  ##### 3. calculate the distance from the origin #####
  for (i in 1:nrow(MetasubDataPreds)){
    MetasubDataPreds[i,"distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), 
                                                                   c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
  }
  
  
  ##### 4. extract samples based on the filter_level#####
  unique(MetasubDataPreds$continent)
  if(filter_level == 'all'){
    df <- MetasubDataPreds %>%
      # filter(sameContinent == FALSE) %>%
      # filter(distance_from_origin > 500) %>%
      select(longitude,latitude,longPred, latPred,sameContinent, country)
  }else if(filter_level == 'notSameContinent500'){
    df <- MetasubDataPreds %>%
      filter(sameContinent == FALSE) %>%
      filter(distance_from_origin > 500) %>%
      select(longitude,latitude,longPred, latPred,sameContinent, country)
    
  }else if(filter_level == 'notSameContinent1000'){
    df <- MetasubDataPreds %>%
      filter(sameContinent == FALSE) %>%
      filter(distance_from_origin > 1000) %>%
      select(longitude,latitude,longPred, latPred,sameContinent, country)
    
  }else if(filter_level == 'notSameContinent'){
    df <- MetasubDataPreds %>%
      filter(sameContinent == FALSE) %>%
      select(longitude,latitude,longPred, latPred,sameContinent, country)
    
  }else{stop('Invalid filter_level input')}
  
  df$color <- NA
  for(i in 1:nrow(df)){
    if(df$sameContinent[i] == T){
      df$color[i] <- "Same continent"
    }else{
      df$color[i] <- "Different continent"

    }
  }
  # print(head(df))
  
  ##### 5. calculate the portion of samples predicted to be in the same continent in continent domain, and assign the cooridnate to draw pie for each continent #####
  # print(pie_continent)
  if(filter_level != 'all'){
    pie_continent <- data.frame(continent=c(unique(MetasubDataPreds$continent), 'Overall'),
                                latitude= NA, longitude = NA,
                                in_continent = NA, not_in_continent = NA)
    
    for(i in c(unique(MetasubDataPreds$continent), 'Overall')){
      if(i == 'Overall'){
        pie_continent$in_continent[which(pie_continent$continent == 'Overall')] <- nrow(MetasubDataPreds[which(MetasubDataPreds$sameContinent == TRUE),])/nrow(MetasubDataPreds)*100
        pie_continent$not_in_continent[which(pie_continent$continent == 'Overall')] <- 100-pie_continent$in_continent[which(pie_continent$continent == 'Overall')]
        
      }else{
        the_sample <- MetasubDataPreds[which(MetasubDataPreds$continent == i),]
        pie_continent$in_continent[which(pie_continent$continent == i)] <- nrow(the_sample[which(the_sample$sameContinent == TRUE),])/nrow(the_sample)*100
        pie_continent$not_in_continent[which(pie_continent$continent == i)] <- 100-pie_continent$in_continent[which(pie_continent$continent == i)]
        
      }
    }
    pie_continent$latitude[which(pie_continent$continent == "Overall")] <- -62
    pie_continent$longitude[which(pie_continent$continent == "Overall")] <- -155
    
  }else{
    pie_continent <- data.frame(continent=unique(MetasubDataPreds$continent),
                                latitude= NA, longitude = NA,
                                in_continent = NA, not_in_continent = NA)
    
    for(i in unique(MetasubDataPreds$continent)){

      the_sample <- MetasubDataPreds[which(MetasubDataPreds$continent == i),]
      pie_continent$in_continent[which(pie_continent$continent == i)] <- nrow(the_sample[which(the_sample$sameContinent == TRUE),])/nrow(the_sample)*100
      pie_continent$not_in_continent[which(pie_continent$continent == i)] <- 100-pie_continent$in_continent[which(pie_continent$continent == i)]
      
    }
    
  }
  pie_continent$latitude[which(pie_continent$continent == "Africa")] <- -12
  pie_continent$longitude[which(pie_continent$continent == "Africa")] <- -4
  pie_continent$latitude[which(pie_continent$continent == "Europe")] <- 73
  pie_continent$longitude[which(pie_continent$continent == "Europe")] <- 40
  pie_continent$latitude[which(pie_continent$continent == "Asia")] <- 44
  pie_continent$longitude[which(pie_continent$continent == "Asia")] <- 156
  pie_continent$latitude[which(pie_continent$continent == "Oceania")] <- -21
  pie_continent$longitude[which(pie_continent$continent == "Oceania")] <- 97
  pie_continent$latitude[which(pie_continent$continent == "Americas")] <- 26
  pie_continent$longitude[which(pie_continent$continent == "Americas")] <- -56
  
  colnames(pie_continent)[4:5] <- c("Same continent", "Different continent")
  
  print(pie_continent)
  
  ##### 6. map visualization with pie chart #####
  cols <- c("Same continent"="steelblue1","Different continent"="violetred1",
            "original geographic coordinate"="blue4", 'predicted geographic coordinate'="firebrick3")
  
  mapWorld <- borders("world", colour="gray50", fill="white")
  p <- df %>% ggplot() + mapWorld+
    geom_point(aes(x = longitude, y = latitude, color = "original geographic coordinate"))+
    geom_point(aes(x = longPred, y = latPred, color = 'predicted geographic coordinate'))+
    geom_segment(aes(x = longitude, y = latitude,
                     xend = longPred, yend = latPred, group = sameContinent, color = color ),
                 arrow = arrow(length = unit(0.2, 'cm')), size = 0.25, alpha = 0.5)+
    geom_scatterpie(aes(x=longitude, y=latitude,  r = 6),
                    data = pie_continent, cols = c("Same continent", "Different continent") )+
    labs(x = "longitude",
         y = "latitude")+
    scale_colour_manual(name="Prediciton",values=cols, 
                        guide = guide_legend(override.aes=aes(fill=NA))) + 
    scale_fill_manual(name="Prediciton",values=cols, guide="none") +
    theme(legend.position = "bottom") 

  ggsave(p, file=paste0(png_path,'_',filter_level,'.png'), dpi = 600, width = 14, height=8)
  
}

```
> 1. AADR set data preparation
  Codes here are perfomed in Bash command. [PLINK 1.9](https://www.cog-genomics.org/plink/1.9/) is required. 
  
  ```console
  # Get overlaps between ancestrial population set and AADR set
  plink --bfile ../Genographic/num_Admixture_reference_pops --extract reich_here.bim --make-bed --out genepool_overlap_SNP --noweb
  
  # keep only overlapped SNPs in AADR set
  plink --bfile reich_here --extract ../Genographic/num_Admixture_reference_pops.bim --make-bed --out reich_here_overlap --noweb
  ```
  
> 2. AIM set curation using AADR set 


* 2.1. Randomly split AADR samples into 2 based on a filtration criteria, do 100 times

The filtration criteria:  

Before data splitting, countires having sample size < 5 should be discarded.  

During splitting, split smaples in each country into 2 sets in same size. If the sample size after splitting < 5, all samples in this country are put into training set. Otherwise, training set and test set will get random samples from this country in same size.   

Codes here are performed in R 4.1.2. 

```r
  # load meta
  meta <- read.csv('Data/meta_table') #nrow(meta)=14008
  # load sample tbl
  fam <- read.table('Data/reich_here_overlap.fam')[,2] 
  fam_file <- read.table('Data/reich_here_overlap.fam')
  meta <- meta[which(meta$Version.ID %in% fam),] 

  # remove countries having samples < 5
  ctry_count <- as.data.frame(table(meta$Country))
  smaller_than_five <- ctry_count$Var1[which(ctry_count$Freq<=5)]
  meta <- meta[-which(meta$Country %in% smaller_than_five ),] 
  ctry_count <- ctry_count[-which(ctry_count$Freq<=5),] 


  for(j in 1:100){
  sample_set1 <- c()
  sample_set2 <- c()
  
  # select half of samples from each country
   for(i in 1:nrow(ctry_count)){
     ctry <- ctry_count$Var1[i]
     ctry_sample <- meta$Version.ID[which(meta$Country == ctry)]
     split_size <- ceiling(length(ctry_sample)/2)
     if(split_size >= 5){ # if sample size after splitting < 5, all samples in this country are put into training set
       sample_set1 <- c(sample_set1, sample(ctry_sample, size = split_size))
       sample_set2 <- c(sample_set2, ctry_sample[-which(ctry_sample %in% sample_set1)])
     }else{ # if not, put into 2 sets
       sample_set1 <- c(sample_set1, ctry_sample)
     }
   }
   if(length(sample_set1) + length(sample_set2) == nrow(meta)){ # ensure no sample missing
     reference_sample <- fam_file[which(fam_file$V2 %in% sample_set1),]
     test_sample <- fam_file[which(fam_file$V2 %in% sample_set2),]
      
     # save them into file, ans save to a corresponding directory
     dir.create(paste0('dt',j))
     write.table(reference_sample, file = paste0('dt',j,'/reference_sample'),
                  quote = F, row.names = F, sep = '\t')
     write.table(test_sample, file = paste0('dt',j,'/test_sample'),
                  quote = F, row.names = F, sep = '\t')
   }else{
     message(nrow(reference_sample))
     message(nrow(test_sample))
     stop(paste0('In iteration ',j,', the sum of reference sample size and test sample size is not equal to the size of Reich dataset'))
   }
  
  
  }

```

* 2.2. For each set of split AADR training and test sets:
  
All R code in section 2.2. can be in one R file *ref_pipeline.R*, and run with arugument passing through this script in Bash command such as:

(Note that *<num>* represents *the number of current set from 100 runs*)

```console
  Rscript --vanilla ref_pipeline.R <num>
```
  
In ref_pipeline.R, start with:
```r
  args <- commandArgs(trailingOnly=TRUE)
  ii <- args[1]
  setwd(paste0('dt',ii))
```
  
to navigate to the directory having split sample sets in the number of current set from 100 runs.

  + 2.2.1. Extract samples for training set and test set
  
  Codes here are performed in R. PLINK 1.9 is required. 
  ```r
     system('plink --bfile ../reich_here_overlap --keep reference_sample --make-bed --out reference_reich --noweb')
     system('plink --bfile ../reich_here_overlap --keep test_sample --make-bed --out test_reich --noweb')

  ```
  + 2.2.2. Prepare training set + run ADMIXTURE in supervised mode for training set
  
  Codes here are performed in R. Both [PLINK 1.07](https://zzz.bwh.harvard.edu/plink/download.shtml) and PLINK 1.9 are required.
  ```r
    system('sh ../baseline_preparation')
  ```
  
  where *baseline_preparation*:
  
  (Note that ADMIXTURE is required, and is prepared in main page named as *admixture32*)
  ```console
    # since the coding of base is different for AADR set and ancestral population set, convert the base in AADR set
    ~/bin/plink-1.07-x86_64/plink --bfile reference_reich --allele1234 --make-bed --out reference_reich_qc --noweb

    # try to merge training set with ancestral population set, will get an error due to different allelic location in 2 sets, will automatically generate a .missnp file
    plink --bfile reference_reich_qc --bmerge ../genepool_overlap.bed ../genepool_overlap.bim ../genepool_overlap.fam  --make-bed --out baseline_overlap --noweb --allow-no-sex

    # remove SNPs in different alleleic location
    plink --bfile ../genepool_overlap --exclude baseline_overlap-merge.missnp  --make-bed --out genepool_overlap_qc
    plink --bfile reference_reich_qc --exclude baseline_overlap-merge.missnp  --make-bed --out reich_here_qc2
    
    # merge genepool to AADR set
    plink --bfile reich_here_qc2 --bmerge genepool_overlap_qc.bed genepool_overlap_qc.bim genepool_overlap_qc.fam  --make-bed --out baseline_overlap --noweb --allow-no-sex
  
    ### To avoid error in ADMIXTURE due to some of samples having all SNPs missing, do quality control for the set
    # Calculate missing rate 
    plink --bfile baseline_overlap --missing --out baseline_overlap --noweb

    # Get the number of SNPs in baseline_overlap
    wc -l baseline_overlap.bim # to get the number of SNPs in baseline_overlap

    # Get samples having all SNPs missing
    cat baseline_overlap.imiss  | awk '{if($4==109627) print $2}' >  baseline_overlap_missing_all_SNPs 

    # Remove collected samples
    cat baseline_overlap.fam | grep -wEf baseline_overlap_missing_all_SNPs > baseline_overlap_removeIndividual.txt  
    plink --bfile baseline_overlap --remove baseline_overlap_removeIndividual.txt --noweb --allow-no-sex --make-bed --out baseline_overlap_qc
  
    # Generate population  file for ADMIXTURE in supervised mode
    cut -f1-2 -d ' ' baseline_overlap_qc.fam > baseline_overlap_qc.pop.txt
    printf '%.0s\n' {1..1756}  > baseline_overlap_qc.pop
    cat baseline_overlap_qc.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> baseline_overlap_qc.pop

    # Run ADMIXTURE in supervised mode
    ../admixture32 baseline_overlap_qc.bed -F 9 -j8

    # Add header to Q file generated from ADMIXTURE
    cat baseline_overlap_qc.fam | cut -d ' ' -f1-2 > training_out_ind_id
    sed -i 's/ /\t/g' training_out_ind_id
    sed -i 's/ /\t/g' baseline_overlap.9.Q
    paste training_out_ind_id baseline_overlap.9.Q > out_Q_training_baseline_<num>
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_training_baseline  
  ```
  
  + 2.2.3. Model training for training set with non-curated SNPs (baseline)
    
  Baseline is used as a reference to see the performance of model with selected AIMs
  ```r
    qfile <- read.table('out_Q_training_baseline', header = T, sep = '\t')

    # Add meta information
    meta <- read.csv('~/Data/meta_table') 
  
    qfile_nogp <- qfile[-which(qfile$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                   'SouthAfrican', 'SouthWestAsian',
                                                   'NativeAmerican', 'Oceanian',
                                                   'SouthEastAsian', 'NorthernEuropean',
                                                   'SubsaharanAfrican')), ] 
    qfile_nogp$Populations<- as.character(qfile_nogp$Populations)
    qfile_nogp_popFilter <- add_meta_reich(qfile_nogp, meta)
    qfile_nogp_popFilter <- droplevels(qfile_nogp_popFilter)
    str(qfile_nogp_popFilter)
    save(qfile_nogp_popFilter, file = 'baseline_qfile.rdata')

    qfile_nogp_popFilter$GRC <- as.character(qfile_nogp_popFilter$GRC)
    if(sum(is.na(qfile_nogp_popFilter$longitude)) > 0){ # avoid the case that any longitude value from meat_table is invalid
      qfile_nogp_popFilter <- qfile_nogp_popFilter[-which(is.na(qfile_nogp_popFilter$longitude)),]
    }
    
    # Model training
    rf_model_training(qfile_nogp_popFilter,tag = ind)
  ```

  + 2.2.4. Make minor allele frequency (MAF) table for each sample 
                              
  Prepare frequency file and PED file for MAF table construction
  ```r
    system("plink --bfile baseline_overlap_qc --recode --tab --out CONVERTReference")
    system("plink --bfile baseline_overlap_qc --freq --noweb")
    system("mv plink.frq reference.frq")

    # If the disk size is highly limited, these files can be removed
    # system('rm baseline_overlap.*')
    # system('rm genepool_overlap*')
    # system('rm reference_reich*')
    # system('rm reich_here*')
    # system('rm CONVERTReference.map')
    # system('rm CONVERTReference.log')
    # system('rm CONVERTReference.nosex')

  ```
                              
  Construct MAF table
  ```r
    # extracted markers from reference
    marker_frq <- read.table('reference.frq')
    marker_ped <- read.table('CONVERTReference.ped', sep = '\t')
    
    metasub_data <- maf_tbl_generation(marker_frq, marker_ped,
                                       'MAF_reference.rdata')
    system('rm CONVERTReference.ped')

  ```
  
  Add meta data to MAF table 
  ```r
  load('MAF_reference.rdata')
  reference_fam <- read.table('reference_sample', sep='\t')[-1,]
  metasub_data <- metasub_data[-which(metasub_data$Populations %in% c('NorthEastAsian', 'Mediterranean',
                                                                      'SouthAfrican', 'SouthWestAsian',
                                                                      'NativeAmerican', 'Oceanian',
                                                                      'SouthEastAsian', 'NorthernEuropean',
                                                                      'SubsaharanAfrican')), ]
  
  print(paste0('nrow of metasub_data after removing gene pools: ',nrow(metasub_data)))
  
  metasub_data$GRC <- NA
  for(i in 1:nrow(metasub_data)){
    metasub_data$GRC[i] <- as.character(reference_fam$V2[which(reference_fam$V1 == metasub_data$Populations[i])])
  }
  print('metasub_data:')
  str(metasub_data)
  
  # Add meta information
  meta <- read.csv('~/Data/meta_table') 
  metasub_data_meta <- add_meta_reich(metasub_data, meta)
  
  if(sum(is.na(metasub_data_meta$longitude)) > 0){
    str(metasub_data_meta)
    stop(paste0('Number of NA longitude: ',sum(is.na(metasub_data_meta$longitude))))
  }else{
    metasub_data_meta$latitude <- as.numeric(metasub_data_meta$latitude)
    metasub_data_meta$longitude <- as.numeric(metasub_data_meta$longitude)
    
    print('metasub_data_meta:')
    str(metasub_data_meta)
    
    save(metasub_data_meta, file='metasub_data_maf_ref.rdata')
    system('rm MAF_reference.rdata')
  }

  ```
  
  + 2.2.5. feature selection with null importance
  ```r
  ##### Data preparation #####
  load('metasub_data_maf_ref.rdata')

  trial_dt <- metasub_data_meta
  trial_dt <- trial_dt[,c(2:(ncol(trial_dt)-4),
                          grep('latitude', colnames(trial_dt)),
                          grep('longitude', colnames(trial_dt)))] # only remains MAF columns, latitude column, and longitude column 

  x <- trial_dt
  features <- colnames(trial_dt)[c(1:(ncol(trial_dt)-2))]
  if(length(features) + 5 != ncol(metasub_data_meta)){ # 5 columns : Populations, GRC, country, latitude, longitude
    stop('Number of length +5 not equal to MAF table') # if the equation does not meet, more/less features were extracted
  }
  
  
  ##### benchmarking generation #####
  benchmark <- null_importance_select(x=x, features= features,
                                      shuffle=F, seed=123)
  print('benchmark:')
  head(benchmark)
  
  save(benchmark, file = 'benchmark.rdata')
  write.table(benchmark$feature, file = 'benchmark.snp', quote = F, row.names = F, col.names = F)
  
  ##### null importance #####
  null_imp_df <- data.frame()
  nb_runs <- 80 # shuffle 80 times
  start_time <- Sys.time()
  dsp <- c() 
  for(i in 1:nb_runs){
    # Get current run importances
    imp_df <- null_importance_select(x=x, features= features,
                                     shuffle=T, seed=123)
    imp_df$run <- i
    # Concat the latest importances with the old ones
    if(nrow(null_imp_df)==0){
      null_imp_df <- imp_df
    }else{
      null_imp_df<- rbind(null_imp_df, imp_df)
    }
    # Display current run and time used
    spent = Sys.time() - start_time
    dsp <- c(dsp,paste0('Done with    ', i,' of ',nb_runs, '    (spent ', spent,')' )) 
    print(dsp)
    
  }
  end_time <- Sys.time()
  time_for_testPreds_ind <- end_time - start_time
  print(time_for_testPreds_ind)
  
  print('null_imp_df:')
  head(null_imp_df)
  
  save(null_imp_df, file = 'null_imp_df.rdata')

  ##### sync features in null importance and benchmark #####
  load('null_imp_df.rdata')
  load('benchmark.rdata')
  null_imp_df.cp <- null_imp_df
  benchmark.cp <- benchmark
  null_imp_fea <- unique(null_imp_df.cp$feature)
  
  # if features in benchmark does not present in features in data frame having 80 shuffle feature importance results, add this benchmark feature.
  for(j in 1:nrow(benchmark.cp)){ 
    current_fea_benchmark <- benchmark.cp$feature[j]
    if(!current_fea_benchmark %in% null_imp_df$feature){
      null_imp_df <- rbind(as.data.frame(null_imp_df),
                           c(current_fea_benchmark, 0, 0, 0))
    }
  }
  if(sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea) != length(unique(null_imp_df$feature))){
    print(paste0('num of features in benchmark not in null_imp + num of features in null_imp: ',sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea)))
    print(paste0('num of features in null_imp after adding', length(unique(null_imp_df$feature))))
    stop('check the equivalence between benchmark$feature and null_imp$feature')
    
  }else{
    # if features in data frame having 80 shuffle feature importance results does not present in features in benchmark, add this feature
    for(i in 1:length(null_imp_fea)){
      current_fea <- null_imp_fea[i]
      if(!current_fea %in% benchmark$feature){
        benchmark <- rbind(as.data.frame(benchmark), c(current_fea, 0, 0))
      }
    }
    
    if(  nrow(benchmark) == length(unique(null_imp_df$feature)) ){
      benchmark$importance_gain <- as.numeric(benchmark$importance_gain)
      benchmark$importance_split <- as.numeric(benchmark$importance_split)
      null_imp_df$importance_gain <- as.numeric(null_imp_df$importance_gain)
      null_imp_df$importance_split <- as.numeric(null_imp_df$importance_split)
      null_imp_df$run <- as.numeric(null_imp_df$run)
      
      save(benchmark, file = 'benchmark_full.rdata')
      save(null_imp_df, file = 'null_imp_df_full.rdata')
      
    }else{
      print('num of features in benchmark: ', nrow(benchmark))
      print('num of features in null_imp: ', length(unique(null_imp_df$feature)))
      stop('num of row in benchmark does not equal to features in null_imp_df')
    }
    
  }
  
  
    ##### calculate the feature scoring for each feature in benchmark #####
  load('benchmark_full.rdata')
  load('null_imp_df_full.rdata')
  
  feature_scores <- data.frame(feature=NA, split_score=NA, gain_score=NA)
  for (f in unique(benchmark$feature)){
    f_null_imps_gain <- null_imp_df$importance_gain[null_imp_df$feature == f]
    f_act_imps_gain <- mean(benchmark$importance_gain[benchmark$feature == f])
    gain_score <- log(1e-10 + f_act_imps_gain / (1 + quantile(f_null_imps_gain, c(.75))[[1]]))  # Avoid divide by zero
    f_null_imps_split <- null_imp_df$importance_split[null_imp_df$feature == f]
    f_act_imps_split <- mean(benchmark$importance_split[benchmark$feature == f])
    split_score <- log(1e-10 + f_act_imps_split / (1 +  quantile(f_null_imps_split, c(.75))[[1]]))  # Avoid divide by zero
    feature_scores<- rbind(feature_scores,
                           c(f, split_score, gain_score))
    
  }
  
  scores_df <- feature_scores[-1,]
  scores_df$split_score <- as.numeric(scores_df$split_score)
  scores_df$gain_score <- as.numeric(scores_df$gain_score)
  
  print('75 percentile scores_df:')
  str(scores_df)
  
  save(scores_df, file = 'scores_df_75.rdata')
  
  #### select the features in top 300 feature score ####
    filter_selected_fea <- scores_df$feature[which(scores_df$gain_score>0 & scores_df$split_score>0)]

    filter_selected_fea.order <- filter_selected_fea[order(filter_selected_fea, decreasing = T),]
    write.table(filter_selected_fea.order[c(1:300)], file = 'split300.snp', quote = F, row.names = F, col.names = F)

  ```
                                 
  + 2.2.6. Extract selected SNPs in training set + Run ADMIXTURE in supervised mode for training set after feature selection
  
  ```r
    load('metasub_data_maf_ref.rdata')

    system("plink --bfile baseline_overlap_qc --extract split300.snp --make-bed --out selected_fea_score")
    # Generate population  file for ADMIXTURE in supervised mode
    system("cut -f1-2 -d ' ' selected_fea_score.fam > selected_fea_score.pop.txt")
    system(paste0("printf '%.0s\n' {1..",nrow(metasub_data_meta),"}  > selected_fea_score.pop")) #if any sample was filtered out previously when running add_meta_reich function in 2.2.4 section, the number here no longer holds. It should always be the number of lines in selected_fea_score.fam 

    system("sh ../aft_fea_sel_command")
    system("rm selected_fea_score*")

  ```
                 
  where *aft_fea_sel_command*:
  ```bash session
     cat selected_fea_score.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> selected_fea_score.pop

      # Run ADMIXTURE in supervised mode
      ~/admixture32 selected_fea_score.bed -F 9 -j8

      # Add header to Q file generated from ADMIXTURE
      cat selected_fea_score.fam | cut -d ' ' -f1-2 > out_ind_id_selected_fea_score
      sed -i 's/ /\t/g' out_ind_id_selected_fea_score
      sed -i 's/ /\t/g' selected_fea_score.9.Q
      paste out_ind_id_selected_fea_score selected_fea_score.9.Q > out_Q_values_ref_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African' out_Q_values_ref_split300
  ```
  
  + 2.2.7. Model training by random forest and prediction for training set
  ```r
    qfile <- read.table('out_Q_values_ref_split300', header = T, sep = '\t')

    # Add meta information
    meta <- read.csv('~/Data/meta_table') 

    qfile_nogp <- qfile[-which(qfile$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                   'SouthAfrican', 'SouthWestAsian',
                                                   'NativeAmerican', 'Oceanian',
                                                   'SouthEastAsian', 'NorthernEuropean',
                                                   'SubsaharanAfrican')), ] 
    qfile_nogp$Populations<- as.character(qfile_nogp$Populations)
    qfile_nogp_popFilter <- add_meta_reich(qfile_nogp, meta)
    qfile_nogp_popFilter <- droplevels(qfile_nogp_popFilter)
    str(qfile_nogp_popFilter)
    save(qfile_nogp_popFilter, file =  'out_Q_values_ref_split300.rdata')

    qfile_nogp_popFilter$GRC <- as.character(qfile_nogp_popFilter$GRC)
    if(sum(is.na(qfile_nogp_popFilter$longitude)) > 0){ # avoid the case that any longitude value from meat_table is invalid
      qfile_nogp_popFilter <- qfile_nogp_popFilter[-which(is.na(qfile_nogp_popFilter$longitude)),]
    }

    rf_model_training(qfile_nogp_popFilter,tag = 'split300')

  ```
                     
* 2.3. Set the selected SNPs from the split set having the smallest distance from the origin in training set

```r
    setwd('~/')
    distance_in_100runs <- read.csv('median_distance_from_the_origin', header = F)
    distance_in_100runs <- distance_in_100runs[order(distance_in_100runs$V2, decreasing = F),]# order based on the median distance from the origin from the smallest to the highest 
    the_best_set <- distance_in_100runs[1,] # the set finally be selected. Its selected SNP set will be the AIM set finally use
    print(the_best_set)
  
    # get the average value of median distance from the origin for 100 runs
    mean(distance_in_100runs[,2])

                   
```
* 2.4. Compare model training methods for the best split set
  
Model training using random forest with extra column and [mGPS](https://github.com/eelhaik/mGPS) (a modified gradient boosting model training method purposed in Elhaik's lab) will be performed here. GPS method is performed in [Elhaik's lab](https://www.nature.com/articles/ncomms4513), and will not show code here. Model training in random forest has been performed previously.
  
Note that *<best_num>* represents *the number from 100 runs for the best split set*

```r
    setwd(paste0('~/dt',<best_num>)) # <best_num> = the_best_set[,1]
    
    load('out_Q_values_ref_split300.rdata')
    split300 <- qfile_nogp_popFilter
    load('out_Q_training_baseline.rdata')
    baseline <- qfile_nogp_popFilter
    
    # prepare coastline data for the adjustment of predicted geographic coordinate
    coastlines <- cbind("x"  = maps::SpatialLines2map(rworldmap::coastsCoarse)$x ,"y" =maps::SpatialLines2map(rworldmap::coastsCoarse)$y)
    coastlines <- coastlines[complete.cases(coastlines),]
    coastlines <- coastlines[coastlines[,1] < 180 ,]
    
    set.seed(18)
    
    ##### model training by mGPS method #####
    # split the set into 5 subsets.
    trainFolds_split300 <-  caret::createFolds(split300$Populations, k = 5, returnTrain = T)
    gp_split300 <- colnames(split300)[-c(1,2,12,13,14)]
    GeoPreds_split300 <- list()
    
    trainFolds_baseline <-  caret::createFolds(baseline$Populations, k = 5, returnTrain = T)
    gp_baseline <- colnames(split300)[-c(1,2,12,13,14)]
    GeoPreds_baseline <- list()
    
    # For each iteration, use 4 subsets as training set, the left one as test set to do model training and prediction. Since there are 5 subsets, iteration is 5 so that all samples are finally tested
    start_time <- Sys.time()
    for (i in 1:5){
      
      train_split300 <- split300[trainFolds_split300[[i]],]
      test_split300 <- split300[-trainFolds_split300[[i]],]
      
      message('mGPS model training is running...')
      start_time.adm <- Sys.time()
      testPreds_split300 <-mGPS(training = train_split300, testing = test_split300, 
                                classTarget = "Populations",variables = gp_split300, 
                                nthread = 8, 
                                hierarchy = c('country', 'Populations', 'latitude', 'longitude'), 
                                coast=coastlines)
      GeoPreds_split300[[i]] <- testPreds_split300
      
      train_baseline <- baseline[trainFolds_baseline[[i]],]
      test_baseline <- baseline[-trainFolds_baseline[[i]],]
      
      start_time.adm <- Sys.time()
      testPreds_baseline <-mGPS(training = train_baseline, testing = test_baseline, 
                                classTarget = "Populations",variables = gp_baseline, 
                                nthread = 8, 
                                hierarchy = c('country', 'Populations', 'latitude', 'longitude'), 
                                coast=coastlines)
      GeoPreds_baseline[[i]] <- testPreds_baseline
      
      message('random forest with extra column is running...')
      
      
      
      end_time.adm <- Sys.time()
      time_for_testPreds <- end_time.adm - start_time.adm
      print(time_for_testPreds)
      
      
    }
    end_time <- Sys.time()
    time_for_testPreds_ind <- end_time - start_time
    print(time_for_testPreds_ind) 
    
    #####Combine these test predictions into one data set #####
    add_preds_split300 <- list()
    add_preds_baseline <- list()
    
    for (i in 1:5){
      
      add_preds_split300[[i]] <- cbind(split300[-trainFolds_split300[[i]],] , 
                                       "cityPred"= GeoPreds_split300[[i]][[1]], 
                                       "latPred" = GeoPreds_split300[[i]][[2]], 
                                       "longPred" = GeoPreds_split300[[i]][[3]] )
      add_preds_baseline[[i]] <- cbind(baseline[-trainFolds_baseline[[i]],] , 
                                       "cityPred"= GeoPreds_baseline[[i]][[1]], 
                                       "latPred" = GeoPreds_baseline[[i]][[2]], 
                                       "longPred" = GeoPreds_baseline[[i]][[3]] )
      
      
      
    }
    
    MetasubDataPreds <- plyr::rbind.fill(add_preds_split300)
    write.csv(MetasubDataPreds,"qfile_predict_mGPS_split300.csv")
    MetasubDataPreds <- plyr::rbind.fill(add_preds_baseline)
    write.csv(MetasubDataPreds,"qfile_predict_mGPS_baseline.csv")
    
    
    
    
    ##### model training by random forest with extra column method #####
    tune_grid <- expand.grid(.mtry = (1:15))
    
    ### for AIMs selected from top300 feature scores in feature selection with null importance
    # Find the extra column by the feature immportance from random forest training
    training_split300 <- split300
    training_split300$rowIndex <- as.numeric(rownames(training_split300))
    variables_split300 <- colnames(split300)[-c(1,2,12,13,14)]
    folds_split300 <- createFolds(training_split300[,'Populations'], k = 5, returnTrain = T)
    trControl_split300 <-  trainControl( # regression
      method = "cv",
      number = 5,
      verboseIter = FALSE,
      returnData = FALSE,
      search = "grid",
      savePredictions = "final",
      allowParallel = F,
      index = folds_split300)
    Xgb_latitude_split300 <- train(x = training_split300[,variables_split300],y = training_split300[,'latitude'],
                                   method = "rf",
                                   trControl = trControl_split300,
                                   tuneGrid = tune_grid)
    l2_train_split300 <- randomForest::randomForest(x = training_split300[,variables_split300],
                                                    y = training_split300[,'longitude'],
                                                    keep.forest=FALSE, importance=TRUE, mtry=c(6))
    name_split300 <- rownames(l2_train_split300$importance)[order(as.data.frame(l2_train_split300$importance)[,1], decreasing = T)][1]
    
    # Visualize the feature importance
    png('split300_feature_importance_plot.png', width = 8,height = 8, units = 'in', res = 600)
    par(mar=c(15,4,2,1), las=2) 
    varImpPlot(l2_train_split300)
    dev.off()
    
    message(paste0('the added column will be: ',name_split300))
    
    # average of the selected column
    the_mean_split300 <- mean(split300[,name_split300]) # 0.2093802
    
    # Add an extra column of the absolute difference between mean of Oceanian and each Oceanian portion
    split300_extra <- split300
    split300_extra$the_abs_diff <- abs(split300[,name_split300] - the_mean_split300)
    
    rf_model_training(split300_extra, tag = 'split300', extraColumn='the_abs_diff')
    
    
    ### for baseline AIMs
    # Find the extra column by the feature importance from random forest training
    training_baseline <- baseline
    training_baseline$rowIndex <- as.numeric(rownames(training_baseline))
    variables_baseline <- colnames(baseline)[-c(1,2,12,13,14)]
    folds_baseline <- createFolds(training_baseline[,'Populations'], k = 5, returnTrain = T)
    trControl_baseline <-  trainControl( # regression
      method = "cv",
      number = 5,
      verboseIter = FALSE,
      returnData = FALSE,
      search = "grid",
      savePredictions = "final",
      allowParallel = F,
      index = folds_baseline)
    Xgb_latitude_baseline <- train(x = training_baseline[,variables_baseline],y = training_baseline[,'latitude'],
                                   method = "rf",
                                   trControl = trControl_baseline,
                                   tuneGrid = tune_grid)
    l2_train_baseline <- randomForest::randomForest(x = training_baseline[,variables_baseline],
                                                    y = training_baseline[,'longitude'],
                                                    keep.forest=FALSE, importance=TRUE, mtry=c(6))
    name_baseline <- rownames(l2_train_baseline$importance)[order(as.data.frame(l2_train_baseline$importance)[,1], decreasing = T)][1]
    
    # Visualize the feature importance
    png('baseline_feature_importance_plot.png', width = 8,height = 8, units = 'in', res = 600)
    par(mar=c(15,4,2,1), las=2) 
    varImpPlot(l2_train_baseline)
    dev.off()
    
    message(paste0('the added column will be: ',name_baseline))
    
    # average of the selected column
    the_mean_baseline <- mean(baseline[,name_baseline]) # 0.2093802
    
    # Add an extra column of the absolute difference between mean of Oceanian and each Oceanian portion
    baseline_extra <- baseline
    baseline_extra$the_abs_diff <- abs(baseline[,name_baseline] - the_mean_baseline)
    
    rf_model_training(baseline_extra, tag = 'baseline', extraColumn='the_abs_diff')
  
```
Use the predicted median distance from the origin as the value to evaluate the performance of 3 methods. The median distance from the origin is calculated by:
```r
  # read saved data frame from a model training method, and named as MetasubDataPreds
  
  # Calculate the distance between the original and the predicted geographic coordinates, added to column 'Distance_from_origin'
  for (i in 1:nrow(MetasubDataPreds)){
    MetasubDataPreds[i,"Distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
  }
  
  # Print the median of distance from origin
  print(median(MetasubDataPreds$Distance_from_origin ))

```

  
  
* 2.5. AIM selection for the test set in the best split set
  
  In this step, most of codes are same to other sections in 2.. However, only the test set in the best split set is used, and no baseline modeling needed.
  
  All following codes in R can be put into *test_pipeline.R*:
  
  (Note that *<best_num>* represents *the number from 100 runs for the best split set*)
  ```r
    setwd(paste0('dt', <best_num>) # <best_num> = the_best_set[,1])
    
    system('sh test_preparation')
  ```
  
  where *test_preparation*:
  ```console
    # since the coding of base is different for AADR set and ancestral population set, convert the base in AADR set
    ~/bin/plink-1.07-x86_64/plink --bfile test_reich --allele1234 --make-bed --out test_reich_qc --noweb

    # try to merge training set with ancestral population set, will get an error due to different allelic location in 2 sets, will automatically generate a .missno file
    plink --bfile test_reich_qc --bmerge ../../genepool_overlap.bed ../../genepool_overlap.bim ../../genepool_overlap.fam  --make-bed --out test_overlap --noweb --allow-no-sex

    # remove SNPs in different alleleic location
    plink --bfile ../../genepool_overlap --exclude genepool_overlap_missnp --make-bed --out genepool_overlap_qcplink --bfile test_reich_qc --exclude test_reich_qc_missnp --make-bed --out reich_here_qc2

    ### To avoid error in ADMIXTURE due to some of samples having all SNPs missing, do quality control for the set
    # Calculate missing rate 
    plink --bfile test_overlap --missing --out test_overlap --noweb

    # Get the number of SNPs in test_overlap
    wc -l test_overlap.bim # to get the number of SNPs in test_overlap

    # Get samples having all SNPs missing
    cat test_overlap.imiss  | awk '{if($4==109627) print $2}' >  test_overlap_missing_all_SNPs 

    # Remove collected samples
    cat test_overlap.fam | grep -wEf test_overlap_missing_all_SNPs > test_overlap_removeIndividual.txt  
    plink --bfile test_overlap --remove test_overlap_removeIndividual.txt --noweb --allow-no-sex --make-bed --out test_overlap_qc
  ```
  
  ```r
    ########## make frequency file and PED file for test set  ##########
    
    system("plink --bfile test_overlap_qc --recode --tab --out CONVERTTest")
    system("plink --bfile test_overlap_qc --freq --noweb")
    system("mv plink.frq test.frq")
    
    # If the disk size is highly limited, these files can be removed
    # system('rm test_overlap.*')
    # system('rm genepool_overlap*')
    # system('rm test_reich*')
    # system('rm reich_here*')
    # system('rm CONVERTTest.map')
    # system('rm CONVERTTest.log')
    # system('rm CONVERTTest.nosex')
    
    ########## Generate minor allele frequency table for test dataset ##########
    ##### Construct MAF table #####
    # extracted markers from reference
    marker_frq <- read.table('test.frq')
    marker_ped <- read.table('CONVERTTest.ped', sep = '\t')
    
    metasub_data <- maf_tbl_generation(marker_frq, marker_ped,
                                       'MAF_test.rdata')
    system('rm CONVERTTest.ped')
    
    
    ##### Add meta data to MAF table #####
    
    load('MAF_test.rdata')
    reference_fam <- read.table('test_sample', sep='\t')[-1,]
    metasub_data <- metasub_data[-which(metasub_data$Populations %in% c('NorthEastAsian', 'Mediterranean',
                                                                        'SouthAfrican', 'SouthWestAsian',
                                                                        'NativeAmerican', 'Oceanian',
                                                                        'SouthEastAsian', 'NorthernEuropean',
                                                                        'SubsaharanAfrican')), ]
    
    print(paste0('nrow of metasub_data after removing gene pools: ',nrow(metasub_data)))
    
    metasub_data$GRC <- NA
    for(i in 1:nrow(metasub_data)){
      metasub_data$GRC[i] <- as.character(reference_fam$V2[which(reference_fam$V1 == metasub_data$Populations[i])])
    }
    print('metasub_data:')
    str(metasub_data)
    
    # Add meta information
    meta <- read.csv('~/Data/meta_table')
    metasub_data_meta <- add_meta_reich(metasub_data, meta)
    
    if(sum(is.na(metasub_data_meta$longitude)) > 0){
      str(metasub_data_meta)
      stop(paste0('Number of NA longitude: ',sum(is.na(metasub_data_meta$longitude))))
    }else{
      metasub_data_meta$latitude <- as.numeric(metasub_data_meta$latitude)
      metasub_data_meta$longitude <- as.numeric(metasub_data_meta$longitude)
      
      print('metasub_data_meta:')
      str(metasub_data_meta)
      
      save(metasub_data_meta, file='metasub_data_maf_test.rdata')
      system('rm MAF_test.rdata')
    }
    
    
    ########## feature selection with null importance for test set ##########
    
    ##### Data preparation #####
    load('metasub_data_maf_test.rdata')
    
    trial_dt <- metasub_data_meta
    trial_dt <- trial_dt[,c(2:(ncol(trial_dt)-4),
                            grep('latitude', colnames(trial_dt)),
                            grep('longitude', colnames(trial_dt)))] # only remains MAF columns, latitude column, and longitude column 
    
    x <- trial_dt
    features <- colnames(trial_dt)[c(1:(ncol(trial_dt)-2))]
    if(length(features) + 5 != ncol(metasub_data_meta)){ # 5 columns : Populations, GRC, country, latitude, longitude
      stop('Number of length +5 not equal to MAF table') # if the equation does not meet, more/less features were extracted
    }
    
    
    ##### benchmarking generation #####
    benchmark <- null_importance_select(x=x, features= features,
                                        shuffle=F, seed=123)
    print('benchmark:')
    head(benchmark)
    
    save(benchmark, file = 'benchmark_test.rdata')
    write.table(benchmark$feature, file = 'benchmark_test.snp', quote = F, row.names = F, col.names = F)
    
    ##### null importance #####
    null_imp_df <- data.frame()
    nb_runs <- 80 # shuffle 80 times
    start_time <- Sys.time()
    dsp <- c() 
    for(i in 1:nb_runs){
      # Get current run importances
      imp_df <- null_importance_select(x=x, features= features,
                                       shuffle=T, seed=123)
      imp_df$run <- i
      # Concat the latest importances with the old ones
      if(nrow(null_imp_df)==0){
        null_imp_df <- imp_df
      }else{
        null_imp_df<- rbind(null_imp_df, imp_df)
      }
      # Display current run and time used
      spent = Sys.time() - start_time
      dsp <- c(dsp,paste0('Done with    ', i,' of ',nb_runs, '    (spent ', spent,')' )) 
      print(dsp)
      
    }
    end_time <- Sys.time()
    time_for_testPreds_ind <- end_time - start_time
    print(time_for_testPreds_ind)
    
    print('null_imp_df:')
    head(null_imp_df)
    
    save(null_imp_df, file = 'null_imp_df_test.rdata')
    
    
    ##### sync features in null importance and benchmark #####
    load('null_imp_df_test.rdata')
    load('benchmark_test.rdata')
    null_imp_df.cp <- null_imp_df
    benchmark.cp <- benchmark
    null_imp_fea <- unique(null_imp_df.cp$feature)
    
    # if features in benchmark does not present in features in data frame having 80 shuffle feature importance results, add this benchmark feature.
    for(j in 1:nrow(benchmark.cp)){ 
      current_fea_benchmark <- benchmark.cp$feature[j]
      if(!current_fea_benchmark %in% null_imp_df$feature){
        null_imp_df <- rbind(as.data.frame(null_imp_df),
                             c(current_fea_benchmark, 0, 0, 0))
      }
    }
    if(sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea) != length(unique(null_imp_df$feature))){
      print(paste0('num of features in benchmark not in null_imp + num of features in null_imp: ',sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea)))
      print(paste0('num of features in null_imp after adding', length(unique(null_imp_df$feature))))
      stop('check the equivalence between benchmark$feature and null_imp$feature')
      
    }else{
      # if features in data frame having 80 shuffle feature importance results does not present in features in benchmark, add this feature
      for(i in 1:length(null_imp_fea)){
        current_fea <- null_imp_fea[i]
        if(!current_fea %in% benchmark$feature){
          benchmark <- rbind(as.data.frame(benchmark), c(current_fea, 0, 0))
        }
      }
      
      if(  nrow(benchmark) == length(unique(null_imp_df$feature)) ){
        benchmark$importance_gain <- as.numeric(benchmark$importance_gain)
        benchmark$importance_split <- as.numeric(benchmark$importance_split)
        null_imp_df$importance_gain <- as.numeric(null_imp_df$importance_gain)
        null_imp_df$importance_split <- as.numeric(null_imp_df$importance_split)
        null_imp_df$run <- as.numeric(null_imp_df$run)
        
        save(benchmark, file = 'benchmark_full_test.rdata')
        save(null_imp_df, file = 'null_imp_df_full_test.rdata')
        
      }else{
        print('num of features in benchmark: ', nrow(benchmark))
        print('num of features in null_imp: ', length(unique(null_imp_df$feature)))
        stop('num of row in benchmark does not equal to features in null_imp_df')
      }
      
    }
    
    
    
    ##### calculate the feature scoring for each feature in benchmark #####
    load('benchmark_full_test.rdata')
    load('null_imp_df_full_test.rdata')
    
    feature_scores <- data.frame(feature=NA, split_score=NA, gain_score=NA)
    for (f in unique(benchmark$feature)){
      f_null_imps_gain <- null_imp_df$importance_gain[null_imp_df$feature == f]
      f_act_imps_gain <- mean(benchmark$importance_gain[benchmark$feature == f])
      gain_score <- log(1e-10 + f_act_imps_gain / (1 + quantile(f_null_imps_gain, c(.75))[[1]]))  # Avoid divide by zero
      f_null_imps_split <- null_imp_df$importance_split[null_imp_df$feature == f]
      f_act_imps_split <- mean(benchmark$importance_split[benchmark$feature == f])
      split_score <- log(1e-10 + f_act_imps_split / (1 +  quantile(f_null_imps_split, c(.75))[[1]]))  # Avoid divide by zero
      feature_scores<- rbind(feature_scores,
                             c(f, split_score, gain_score))
      
    }
    
    scores_df <- feature_scores[-1,]
    scores_df$split_score <- as.numeric(scores_df$split_score)
    scores_df$gain_score <- as.numeric(scores_df$gain_score)
    
    print('75 percentile scores_df:')
    str(scores_df)
    
    save(scores_df, file = 'scores_df_75_test.rdata')
    
    #### select the features in top 300 feature score ####
    filter_selected_fea <- scores_df$feature[which(scores_df$gain_score>0 & scores_df$split_score>0)]
    
    filter_selected_fea.order <- filter_selected_fea[order(filter_selected_fea, decreasing = T),]
    write.table(filter_selected_fea.order[c(1:300)], file = 'split300_test.snp', quote = F, row.names = F, col.names = F)
    
    
    
    
    
    ########## Extract selected SNPs in training set + Run ADMIXTURE in supervised mode for training set after feature selection  ##########
    
    # extract selected SNPs from the training set
    load('metasub_data_maf_test.rdata')
    
    system("plink --bfile test_overlap_qc --extract split300_test.snp --make-bed --out selected_fea_score_test")
  ```                                
                                 
* 2.6. Confirm the performance of curated AIM set using AADR set 
  
  3 AIM sets for training set and test set will be tested:
                                 
  1. baseline
        
  2. AIMs found in the benchamrk of feature selection with null importance (benchmark)
                                 
  3. AIMs in top 300 feature scores from feature selection with null importance (split300)
                                 
  ```console
     # since the coding of base is different for AADR set and ancestral population set, convert the base in AADR set
      ~/bin/plink-1.07-x86_64/plink --bfile ../reich_here_overlap --allele1234 --make-bed --out reich_here_overlap_qc --noweb

      # try to merge training set with ancestral population set, will get an error due to different allelic location in 2 sets, will automatically generate a .missnp file
      plink --bfile reich_here_overlap_qc --bmerge ../genepool_overlap.bed ../../genepool_overlap.bim ../../genepool_overlap.fam  --make-bed --out baseline_overlap --noweb --allow-no-sex

      # remove SNPs in different alleleic location
      plink --bfile ../../genepool_overlap --exclude genepool_overlap_missnp --make-bed --out genepool_overlap_qc
      plink --bfile reference_reich_qc --exclude reference_reich_qc_missnp --make-bed --out reich_here_qc2                            
  ```
                                 
    + Use test set samples and selected AIMs to train and predict the training set 
                                 
    Create corresponding directory for the following the test, and naviagte into the directory
    ```r
       setwd('~/')
       system('mkdir test_train')
       setwd(paste0('~/dt',<best_num>,'/test_train')) # <best_num> = the_best_set[,1]     
       system('sh test_train')
    ```
    where *test_train*:  
    (Prepare training set and test set + run ADMIXTURE in supervised mode for all AIM sets)
    ```console
      ## use test set to model training set

      # extract samples from test set in best split set as samples in this training set
      plink --bfile ../reich_here_overlap_qc2 --keep ../test_sample --make-bed --out test_training

      # Since there was no samples having all SNPs missing (i.e no sample removed in quality control step), the quality control step is skipped here
      plink --bfile test_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out test_training_baseline_overlap --allow-no-sex

      # run ADMIXTURE for this training set with all AIMs as baseline
      cut -f1-2 -d ' ' test_training_baseline_overlap.fam > test_training_baseline_overlap.pop.txt

      printf '%.0s\n' {1..1621} > test_training_baseline_overlap.pop

      cat test_training_baseline_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_baseline_overlap.pop

      ~/admixture32 test_training_baseline_overlap.bed -F 9 -j8
      cat test_training_baseline_overlap.fam | cut -d ' ' -f1-2 > test_training_out_ind_id
      sed -i 's/ /\t/g' test_training_out_ind_id
      sed -i 's/ /\t/g' test_training_baseline_overlap.9.Q
      paste test_training_out_ind_id test_training_baseline_overlap.9.Q > out_Q_test_training_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_baseline

      # extract AIMs selected from the benchmark in feature selection with null importance, and run ADMIXTURE for this training set with these AIMs
      plink --bfile test_training_baseline_overlap --extract ../benchmark_test.snp  --make-bed --out test_training_selected_bench --noweb

      cut -f1-2 -d ' ' test_training_selected_bench.fam > test_training_selected_bench.pop.txt
      printf '%.0s\n' {1..1621}  > test_training_selected_bench.pop
      wc -l test_training_selected_bench.pop

      cat test_training_selected_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_selected_bench.pop
      wc -l test_training_selected_bench.pop

      ~/admixture32 test_training_selected_bench.bed -F 9 -j8
      cat test_training_selected_bench.fam | cut -d ' ' -f1-2 > test_training_out_ind_id_bench
      sed -i 's/ /\t/g' test_training_out_ind_id_bench
      sed -i 's/ /\t/g' test_training_selected_bench.9.Q
      paste test_training_out_ind_id_bench test_training_selected_bench.9.Q > out_Q_test_training_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_bench

      # extract AIMs selected from top300 feature scores in feature selection with null importance, and run ADMIXTURE for this training set with these AIMs

      plink --bfile test_training_baseline_overlap --extract ../split300_test.snp  --make-bed --out test_training_selected_split300 --noweb

      cut -f1-2 -d ' ' test_training_selected_split300.fam > test_training_selected_split300.pop.txt
      printf '%.0s\n' {1..1621}  > test_training_selected_split300.pop

      cat test_training_selected_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_selected_split300.pop

      ~/admixture32 test_training_selected_split300.bed -F 9 -j8
      cat test_training_selected_split300.fam | cut -d ' ' -f1-2 > test_training_out_ind_id_split300
      sed -i 's/ /\t/g' test_training_out_ind_id_split300
      sed -i 's/ /\t/g' test_training_selected_split300.9.Q
      paste test_training_out_ind_id_split300 test_training_selected_split300.9.Q > out_Q_test_training_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_split300






      # extract samples from training set in best split set as samples in this test set
      plink --bfile ../reich_here_overlap_qc2 --keep ../reference_sample --make-bed --out test_test

      plink --bfile test_test --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out test_test_baseline_overlap --allow-no-sex

      # run ADMIXTURE for this test set with all AIMs as baseline
      cut -f1-2 -d ' ' test_test_baseline_overlap.fam > test_test_baseline_overlap.pop.txt

      printf '%.0s\n' {1..1756} > test_test_baseline_overlap.pop

      cat test_test_baseline_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_baseline_overlap.pop

      ~/admixture32 test_test_baseline_overlap.bed -F 9 -j8
      cat test_test_baseline_overlap.fam | cut -d ' ' -f1-2 > test_test_out_ind_id
      sed -i 's/ /\t/g' test_test_out_ind_id
      sed -i 's/ /\t/g' test_test_baseline_overlap.9.Q
      paste test_test_out_ind_id test_test_baseline_overlap.9.Q > out_Q_test_test_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_baseline


      # extract AIMs selected from the benchmark in feature selection with null importance, and run ADMIXTURE for this test set with these AIMs
      plink --bfile test_test_baseline_overlap --extract ../benchmark_test.snp  --make-bed --out test_test_selected_bench --noweb

      cut -f1-2 -d ' ' test_test_selected_bench.fam > test_test_selected_bench.pop.txt
      printf '%.0s\n' {1..1756}  > test_test_selected_bench.pop
      wc -l test_test_selected_bench.pop

      cat test_test_selected_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_selected_bench.pop
      wc -l test_test_selected_bench.pop

      ~/admixture32 test_test_selected_bench.bed -F 9 -j8
      cat test_test_selected_bench.fam | cut -d ' ' -f1-2 > test_test_out_ind_id_bench
      sed -i 's/ /\t/g' test_test_out_ind_id_bench
      sed -i 's/ /\t/g' test_test_selected_bench.9.Q
      paste test_test_out_ind_id_bench test_test_selected_bench.9.Q > out_Q_test_test_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_bench

      # extract AIMs selected from top300 feature scores in feature selection with null importance, and run ADMIXTURE for this test set with these AIMs

      plink --bfile test_test_baseline_overlap --extract ../split300_test.snp  --make-bed --out test_test_selected_split300 --noweb

      cut -f1-2 -d ' ' test_test_selected_split300.fam > test_test_selected_split300.pop.txt
      printf '%.0s\n' {1..1756}  > test_test_selected_split300.pop

      cat test_test_selected_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_selected_split300.pop

      ~/admixture32 test_test_selected_split300.bed -F 9 -j8
      cat test_test_selected_split300.fam | cut -d ' ' -f1-2 > test_test_out_ind_id_split300
      sed -i 's/ /\t/g' test_test_out_ind_id_split300
      sed -i 's/ /\t/g' test_test_selected_split300.9.Q
      paste test_test_out_ind_id_split300 test_test_selected_split300.9.Q > out_Q_test_test_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_split300


      Rscript --vanilla ../run_rf_train_test.R test

    ```
  
    where *run_rf_train_test.R*:
    ```r
    args <- commandArgs(trailingOnly=TRUE)
    iteration <- c('baseline', 'bench', 'split300')
    ind <- args[[1]]
    
    for(ite in iteration){ # baseline; benchmark; split300
    
      qfile_train <- read.table(paste0('out_Q_',ind,'_training_',ite), header = T, sep = '\t') 
      qfile_test <- read.table(paste0('out_Q_',ind,'_test_',ite), header = T, sep = '\t')
      
      # Add meta information for training set
      meta <- read.csv('~/Data/meta_table')
    
      qfile_train_nogp <- qfile_train[-which(qfile_train$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                           'SouthAfrican', 'SouthWestAsian',
                                                                           'NativeAmerican', 'Oceanian',
                                                                           'SouthEastAsian', 'NorthernEuropean',
                                                                           'SubsaharanAfrican')), ]
      qfile_train_nogp$Populations<- as.character(qfile_train_nogp$Populations)
      qfile_train_nogp_popFilter <- add_meta_reich(qfile_train_nogp, meta)
      qfile_train_nogp_popFilter <- droplevels(qfile_train_nogp_popFilter)
      print('qfile_train_nogp_popFilter:')
      str(qfile_train_nogp_popFilter)
      
      # Add meta information for test set
      qfile_test_nogp <- qfile_test[-which(qfile_test$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                        'SouthAfrican', 'SouthWestAsian',
                                                                        'NativeAmerican', 'Oceanian',
                                                                        'SouthEastAsian', 'NorthernEuropean',
                                                                        'SubsaharanAfrican')), ]
      qfile_test_nogp$Populations<- as.character(qfile_test_nogp$Populations)
      qfile_test_nogp_popFilter <- add_meta_reich(qfile_test_nogp, meta)
      qfile_test_nogp_popFilter <- droplevels(qfile_test_nogp_popFilter)
      print('qfile_test_nogp_popFilter:')
      str(qfile_test_nogp_popFilter)
      
      qfile_train_nogp_popFilter$GRC <- as.character(qfile_train_nogp_popFilter$GRC)
      qfile_test_nogp_popFilter$GRC <- as.character(qfile_test_nogp_popFilter$GRC)
      
      # In case any invalid longitude value in meta data leads to error in model training, remove corresponding samples
      if(sum(is.na(qfile_train_nogp_popFilter$longitude)) > 0){
        qfile_train_nogp_popFilter <- qfile_train_nogp_popFilter[-which(is.na(qfile_train_nogp_popFilter$longitude)),]
      }
      if(sum(is.na(qfile_test_nogp_popFilter$longitude)) > 0){
        qfile_test_nogp_popFilter <- qfile_test_nogp_popFilter[-which(is.na(qfile_test_nogp_popFilter$longitude)),]
      }
      
      # Model training and prediction
      rf_model_training_train_test(qfile_train_nogp_popFilter, qfile_test_nogp_popFilter, tag = c(ind, ite))
      
    }

    ```
    + Use training set samples and selected AIMs to train and predict test set 
                                 
    Create corresponding directory for the following the test, and naviagte into the directory
    ```r
       system('mkdir ../train_test')
       setwd(paste0('~/dt', <best_num> ,'/train_test')) # <best_num> = the_best_set[,1]     
       system('sh train_test')
    ```
    where *train_test*:  
    (Prepare training set and test set + run ADMIXTURE in supervised mode for all AIM sets)
    ```console  
       ## use training set to model test set 

      # extract samples from test set in best split set as samples in this training set
      plink --bfile ../reich_here_overlap_qc2 --keep ../reference_sample --make-bed --out test_training

      # Since there was no samples having all SNPs missing (i.e no sample removed in quality control step), the quality control step is skipped here
      plink --bfile test_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out test_training_baseline_overlap --allow-no-sex

      cut -f1-2 -d ' ' test_training_baseline_overlap.fam > test_training_baseline_overlap.pop.txt

      printf '%.0s\n' {1..1756} > test_training_baseline_overlap.pop

      cat test_training_baseline_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_baseline_overlap.pop

      ~/admixture32 test_training_baseline_overlap.bed -F 9 -j8
      cat test_training_baseline_overlap.fam | cut -d ' ' -f1-2 > test_training_out_ind_id
      sed -i 's/ /\t/g' test_training_out_ind_id
      sed -i 's/ /\t/g' test_training_baseline_overlap.9.Q
      paste test_training_out_ind_id test_training_baseline_overlap.9.Q > out_Q_test_training_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_baseline

      # extract AIMs selected from the benchmark in feature selection with null importance, and run ADMIXTURE for this training set with these AIMs
      plink --bfile test_training_baseline_overlap --extract ../benchmark.snp  --make-bed --out test_training_selected_bench --noweb

      cut -f1-2 -d ' ' test_training_selected_bench.fam > test_training_selected_bench.pop.txt
      printf '%.0s\n' {1..1756}  > test_training_selected_bench.pop
      wc -l test_training_selected_bench.pop

      cat test_training_selected_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_selected_bench.pop
      wc -l test_training_selected_bench.pop

      ~/admixture32 test_training_selected_bench.bed -F 9 -j8
      cat test_training_selected_bench.fam | cut -d ' ' -f1-2 > test_training_out_ind_id_bench
      sed -i 's/ /\t/g' test_training_out_ind_id_bench
      sed -i 's/ /\t/g' test_training_selected_bench.9.Q
      paste test_training_out_ind_id_bench test_training_selected_bench.9.Q > out_Q_test_training_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_bench

      # extract AIMs selected from top300 feature scores in feature selection with null importance, and run ADMIXTURE for this training set with these AIMs

      plink --bfile test_training_baseline_overlap --extract ../split300.snp  --make-bed --out test_training_selected_split300 --noweb

      cut -f1-2 -d ' ' test_training_selected_split300.fam > test_training_selected_split300.pop.txt
      printf '%.0s\n' {1..1756}  > test_training_selected_split300.pop

      cat test_training_selected_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_training_selected_split300.pop

      ~/admixture32 test_training_selected_split300.bed -F 9 -j8
      cat test_training_selected_split300.fam | cut -d ' ' -f1-2 > test_training_out_ind_id_split300
      sed -i 's/ /\t/g' test_training_out_ind_id_split300
      sed -i 's/ /\t/g' test_training_selected_split300.9.Q
      paste test_training_out_ind_id_split300 test_training_selected_split300.9.Q > out_Q_test_training_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_training_split300






      # extract samples from training set in best split set as samples in this test set
      plink --bfile ../reich_here_overlap_qc2 --keep sample_feature/test_sample --make-bed --out test_test

      plink --bfile test_test --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out test_test_baseline_overlap --allow-no-sex

      # run ADMIXTURE for this test set with all AIMs as baseline
      cut -f1-2 -d ' ' test_test_baseline_overlap.fam > test_test_baseline_overlap.pop.txt

      printf '%.0s\n' {1..1621} > test_test_baseline_overlap.pop

      cat test_test_baseline_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_baseline_overlap.pop

      ~/admixture32 test_test_baseline_overlap.bed -F 9 -j8
      cat test_test_baseline_overlap.fam | cut -d ' ' -f1-2 > test_test_out_ind_id
      sed -i 's/ /\t/g' test_test_out_ind_id
      sed -i 's/ /\t/g' test_test_baseline_overlap.9.Q
      paste test_test_out_ind_id test_test_baseline_overlap.9.Q > out_Q_test_test_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_baseline

      # extract AIMs selected from the benchmark in feature selection with null importance, and run ADMIXTURE for this test set with these AIMs
      plink --bfile test_test_baseline_overlap --extract ../benchmark.snp  --make-bed --out test_test_selected_bench --noweb

      cut -f1-2 -d ' ' test_test_selected_bench.fam > test_test_selected_bench.pop.txt
      printf '%.0s\n' {1..1621}  > test_test_selected_bench.pop
      wc -l test_test_selected_bench.pop

      cat test_test_selected_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_selected_bench.pop
      wc -l test_test_selected_bench.pop

      ~/admixture32 test_test_selected_bench.bed -F 9 -j8
      cat test_test_selected_bench.fam | cut -d ' ' -f1-2 > test_test_out_ind_id_bench
      sed -i 's/ /\t/g' test_test_out_ind_id_bench
      sed -i 's/ /\t/g' test_test_selected_bench.9.Q
      paste test_test_out_ind_id_bench test_test_selected_bench.9.Q > out_Q_test_test_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_bench

      # extract AIMs selected from top300 feature scores in feature selection with null importance, and run ADMIXTURE for this test set with these AIMs

      plink --bfile test_test_baseline_overlap --extract ../split300.snp  --make-bed --out test_test_selected_split300 --noweb

      cut -f1-2 -d ' ' test_test_selected_split300.fam > test_test_selected_split300.pop.txt
      printf '%.0s\n' {1..1621}  > test_test_selected_split300.pop

      cat test_test_selected_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_test_selected_split300.pop

      ~/admixture32 test_test_selected_split300.bed -F 9 -j8
      cat test_test_selected_split300.fam | cut -d ' ' -f1-2 > test_test_out_ind_id_split300
      sed -i 's/ /\t/g' test_test_out_ind_id_split300
      sed -i 's/ /\t/g' test_test_selected_split300.9.Q
      paste test_test_out_ind_id_split300 test_test_selected_split300.9.Q > out_Q_test_test_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_test_split300


      Rscript --vanilla ../run_rf_train_test.R test
    ```
    
    where run_rf_train_test.R is same to the one in *using test set to predict training set*
  
  * 2.7. Confirm the performance of curated AIM sets using a new test set (output_645)
  
  Since we have 2 sets from the best split set, the new test set will be tested based on each model.
  
  ```console
     #  use test to train output_645

      # get overlapped SNPs between output_645 and gene pool set
      plink --bfile ../../Genographic/output_645 --extract ../../Genographic/num_Admixture_reference_pops.bim --make-bed --out output_overlap 

      # merge output_645 to gene pool set
      plink --bfile ../../Genographic/num_Admixture_reference_pops --extract output_overlap.bim --make-bed --out genepool_overlap 

      plink --bfile output_overlap --bmerge genepool_overlap.bed genepool_overlap.bim genepool_overlap.fam --make-bed --out output_genepool --allow-no-sex

      # get overlapped SNPs between output_genepool and AADR set => baseline SNPs
      plink --bfile output_genepool --extract ../reich_here_overlap_qc2.bim --make-bed --out baseline_overlap



      # extract test set from AADR set as training set for output_645
      plink --bfile ../reich_here_overlap_qc2 --keep sample_feature/test_sample --make-bed --out test_training

      # merge test_training to gene pool set
      plink --bfile test_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam --make-bed --out test_genepool --allow-no-sex

      # get the overlap SNPs for both output645 and AADR set for training set
      plink --bfile test_genepool --extract baseline_overlap.bim --make-bed --out test_overlap





      # baseline admixture for test_training (training set)

      cut -f1-2 -d ' ' test_overlap.fam > test_overlap.pop.txt

      printf '%.0s\n' {1..1621} > test_overlap.pop

      cat test_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_overlap.pop

      ~/admixture32 test_overlap.bed -F 9 -j8
      cat test_overlap.fam | cut -d ' ' -f1-2 > test_overlap_out_ind_id
      sed -i 's/ /\t/g' test_overlap_out_ind_id
      sed -i 's/ /\t/g' test_overlap.9.Q
      paste test_overlap_out_ind_id test_overlap.9.Q > out_Q_test_overlap_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_overlap_baseline

      # baseline admixture for baseline_overlap (test set)

      cut -f1-2 -d ' ' baseline_overlap.fam > baseline_overlap.pop.txt

    sed 's/.*GRC.*/ /g' baseline_overlap.pop.txt | sed 's/[0-9]//g' | cut -f1 -d' ' > baseline_overlap.pop


      ~/admixture32 baseline_overlap.bed -F 9 -j8
      cat baseline_overlap.fam | cut -d ' ' -f1-2 > baseline_overlap_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap.9.Q
      paste baseline_overlap_out_ind_id baseline_overlap.9.Q > out_Q_baseline_overlap_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_baseline_overlap_baseline


      # benchmark admixture for test_training (training set)

      plink --bfile test_overlap --extract sample_feature/benchmark_test.snp  --make-bed --out test_overlap_bench

      cut -f1-2 -d ' ' test_overlap_bench.fam > test_overlap_bench.pop.txt

      printf '%.0s\n' {1..1621} > test_overlap_bench.pop

      cat test_overlap_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_overlap_bench.pop

      ~/admixture32 test_overlap_bench.bed -F 9 -j8
      cat test_overlap_bench.fam | cut -d ' ' -f1-2 > test_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' test_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' test_overlap_bench.9.Q
      paste test_overlap_bench_out_ind_id test_overlap_bench.9.Q > out_Q_test_overlap_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_overlap_bench

      # benchmark admixture for baseline_overlap (test set)

      plink --bfile baseline_overlap --extract sample_feature/benchmark_test.snp  --make-bed --out baseline_overlap_bench

      cut -f1-2 -d ' ' baseline_overlap_bench.fam > baseline_overlap_bench.pop.txt

      sed 's/.*GRC.*/ /g' baseline_overlap_bench.pop.txt | sed 's/[0-9]//g' | cut -f1 -d' ' > baseline_overlap_bench.pop


      ~/admixture32 baseline_overlap_bench.bed -F 9 -j8
      cat baseline_overlap_bench.fam | cut -d ' ' -f1-2 > baseline_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap_bench.9.Q
      paste baseline_overlap_bench_out_ind_id baseline_overlap_bench.9.Q > out_Q_baseline_overlap_bench
sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_baseline_overlap_bench


      # split300 admixture for test_training (training set)

      plink --bfile test_overlap --extract sample_feature/socres_df.split.top300_test.snp  --make-bed --out test_overlap_split300

      cut -f1-2 -d ' ' test_overlap_split300.fam > test_overlap_split300.pop.txt

      printf '%.0s\n' {1..1621} > test_overlap_split300.pop

      cat test_overlap_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> test_overlap_split300.pop

      ~/admixture32 test_overlap_split300.bed -F 9 -j8
      cat test_overlap_split300.fam | cut -d ' ' -f1-2 > test_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' test_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' test_overlap_split300.9.Q
      paste test_overlap_split300_out_ind_id test_overlap_split300.9.Q > out_Q_test_overlap_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_test_overlap_split300

      # split300 admixture for baseline_overlap (test set)

      plink --bfile baseline_overlap --extract sample_feature/socres_df.split.top300_test.snp  --make-bed --out baseline_overlap_split300

      cut -f1-2 -d ' ' baseline_overlap_split300.fam > baseline_overlap_split300.pop.txt

      sed 's/.*GRC.*/ /g' baseline_overlap_split300.pop.txt | sed 's/[0-9]//g' | cut -f1 -d' ' >  baseline_overlap_split300.pop


      ~/admixture32 baseline_overlap_split300.bed -F 9 -j8
      cat baseline_overlap_split300.fam | cut -d ' ' -f1-2 > baseline_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' baseline_overlap_split300.9.Q
      paste baseline_overlap_split300_out_ind_id baseline_overlap_split300.9.Q > out_Q_baseline_overlap_split300

      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_baseline_overlap_split300



      Rscript --vanilla run_rf_output.R test baseline




      #  use train to train output_645

      # extract test set from AADR set as training set for output_645
      plink --bfile ../reich_here_overlap_qc2 --keep sample_feature/reference_sample --make-bed --out train_training

      # merge train_training to gene pool set
      plink --bfile train_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam --make-bed --out train_genepool --allow-no-sex

      # get the overlap SNPs for both output645 and AADR set for training set
      plink --bfile train_genepool --extract baseline_overlap.bim --make-bed --out train_overlap


      # baseline admixture for train_training (training set)

      cut -f1-2 -d ' ' train_overlap.fam > train_overlap.pop.txt

      printf '%.0s\n' {1..1756} > train_overlap.pop

      cat train_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> train_overlap.pop

      ~/admixture32 train_overlap.bed -F 9 -j8
      cat train_overlap.fam | cut -d ' ' -f1-2 > train_overlap_out_ind_id
      sed -i 's/ /\t/g' train_overlap_out_ind_id
      sed -i 's/ /\t/g' train_overlap.9.Q
      paste train_overlap_out_ind_id train_overlap.9.Q > out_Q_train_overlap_baseline
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_train_overlap_baseline

      # baseline admixture for baseline_overlap (test set)

      cp out_Q_baseline_overlap_baseline out_Q_baseline_test_overlap_baseline


      # benchmark admixture for train_training (training set)

      plink --bfile train_overlap --extract sample_feature/benchmark.snp  --make-bed --out train_overlap_bench

      cut -f1-2 -d ' ' train_overlap_bench.fam > train_overlap_bench.pop.txt

      printf '%.0s\n' {1..1756} > train_overlap_bench.pop

      cat train_overlap_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> train_overlap_bench.pop

      ~/admixture32 train_overlap_bench.bed -F 9 -j8
      cat train_overlap_bench.fam | cut -d ' ' -f1-2 > train_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' train_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' train_overlap_bench.9.Q
      paste train_overlap_bench_out_ind_id train_overlap_bench.9.Q > out_Q_train_overlap_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_train_overlap_bench

      # benchmark admixture for baseline_overlap (test set)

      plink --bfile baseline_overlap --extract sample_feature/benchmark.snp  --make-bed --out baseline_test_overlap_bench

      cut -f1-2 -d ' ' baseline_test_overlap_bench.fam > baseline_test_overlap_bench.pop.txt

      sed 's/.*GRC.*/ /g' baseline_test_overlap_bench.pop.txt | sed 's/[0-9]//g' | cut -f1 -d' ' > baseline_test_overlap_bench.pop


      ~/admixture32 baseline_test_overlap_bench.bed -F 9 -j8
      cat baseline_test_overlap_bench.fam | cut -d ' ' -f1-2 > baseline_test_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' baseline_test_overlap_bench_out_ind_id
      sed -i 's/ /\t/g' baseline_test_overlap_bench.9.Q
      paste baseline_test_overlap_bench_out_ind_id baseline_test_overlap_bench.9.Q > out_Q_baseline_test_overlap_bench
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_baseline_test_overlap_bench


      # split300 admixture for train_overlap (training set)

      plink --bfile train_overlap --extract sample_feature/socres_df.split.top300.snp  --make-bed --out train_overlap_split300

      cut -f1-2 -d ' ' train_overlap_split300.fam > train_overlap_split300.pop.txt

      printf '%.0s\n' {1..1756} > train_overlap_split300.pop

      cat train_overlap_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> train_overlap_split300.pop

      ~/admixture32 train_overlap_split300.bed -F 9 -j8
      cat train_overlap_split300.fam | cut -d ' ' -f1-2 > train_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' train_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' train_overlap_split300.9.Q
      paste train_overlap_split300_out_ind_id train_overlap_split300.9.Q > out_Q_train_overlap_split300
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_train_overlap_split300

      # split300 admixture for baseline_overlap (test set)

      plink --bfile baseline_overlap --extract sample_feature/socres_df.split.top300.snp  --make-bed --out baseline_test_overlap_split300

      cut -f1-2 -d ' ' baseline_test_overlap_split300.fam > baseline_test_overlap_split300.pop.txt

      sed 's/.*GRC.*/ /g' baseline_test_overlap_split300.pop.txt | sed 's/[0-9]//g' | cut -f1 -d' ' >  baseline_test_overlap_split300.pop


      ~/admixture32 baseline_test_overlap_split300.bed -F 9 -j8
      cat baseline_test_overlap_split300.fam | cut -d ' ' -f1-2 > baseline_test_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' baseline_test_overlap_split300_out_ind_id
      sed -i 's/ /\t/g' baseline_test_overlap_split300.9.Q
      paste baseline_test_overlap_split300_out_ind_id baseline_test_overlap_split300.9.Q > out_Q_baseline_test_overlap_split300

      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_baseline_test_overlap_split300



      Rscript --vanilla run_rf_output.R train baseline_test
 
  ```
  
  where *run_rf_output.R*:
  ```r
    args <- commandArgs(trailingOnly=TRUE)
    iteration <- c('baseline', 'bench', 'split300')
    train <- args[1]
    test <- args[2]
    
    for(ite in iteration){ # baseline; benchmark; split300
      message(paste('==========',train,ite,'=========='))
      
      qfile_train <- read.table(paste0('out_Q_',train,'_overlap_',ite), header = T, sep = '\t')
      qfile_test <- read.table(paste0('out_Q_',test,'_overlap_',ite), header = T, sep = '\t')
      
      # Add meta information
      meta <- read.csv('~/Data/meta_table') #nrow(meta)=14008
    
      # for training set
      qfile_train_nogp <- qfile_train[-which(qfile_train$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                           'SouthAfrican', 'SouthWestAsian',
                                                                           'NativeAmerican', 'Oceanian',
                                                                           'SouthEastAsian', 'NorthernEuropean',
                                                                           'SubsaharanAfrican')), ]
      qfile_train_nogp$Populations<- as.character(qfile_train_nogp$Populations)
      qfile_train_nogp_popFilter <- add_meta_reich(qfile_train_nogp, meta)
      qfile_train_nogp_popFilter <- droplevels(qfile_train_nogp_popFilter)
      print('qfile_train_nogp_popFilter:')
      str(qfile_train_nogp_popFilter)
      
      # for test set
      qfile_test_nogp <- qfile_test[-which(qfile_test$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                        'SouthAfrican', 'SouthWestAsian',
                                                                        'NativeAmerican', 'Oceanian',
                                                                        'SouthEastAsian', 'NorthernEuropean',
                                                                        'SubsaharanAfrican')), ]
      qfile_test_nogp$Populations<- as.character(qfile_test_nogp$Populations)
      meta <- openxlsx::read.xlsx('population_metasub.xlsx')
      qfile_test_nogp_popFilter <- add_meta_ref(qfile_test_nogp, meta)
      qfile_test_nogp_popFilter <- droplevels(qfile_test_nogp_popFilter)
      print('qfile_test_nogp_popFilter:')
      str(qfile_test_nogp_popFilter)
      qfile_train_nogp_popFilter$GRC <- as.character(qfile_train_nogp_popFilter$GRC)
      if(sum(is.na(qfile_train_nogp_popFilter$longitude)) > 0){
        qfile_train_nogp_popFilter <- qfile_train_nogp_popFilter[-which(is.na(qfile_train_nogp_popFilter$longitude)),]
      }
      qfile_test_nogp_popFilter$GRC <- as.character(qfile_test_nogp_popFilter$GRC)
      if(sum(is.na(qfile_test_nogp_popFilter$longitude)) > 0){
        qfile_test_nogp_popFilter <- qfile_test_nogp_popFilter[-which(is.na(qfile_test_nogp_popFilter$longitude)),]
      }
      
      ### Model training ###
      rf_model_training_train_test(qfile_train_nogp_popFilter, qfile_test_nogp_popFilter, tag = c(train, ite))
      
      
    }

  ```

  
  
> 3. Leave-one-out procedure
  
  In leave-one-out procedure, all sample are set as test set once against the rest of samples as training set. 
  
  * Create a new directory for this procedure, and organize the directory
  
  Note that <best_num> represents the number from 100 runs for the best split set
  ```r
    # create a directory for this process
    setwd('~/')
    system('mkdir LOO')
    setwd('~/LOO')
    
    # copy samples and selected AIM sets to a directory
    system('mkdir sample_feature')
    system(paste0('cp ../dt', <best_num> ,'/reference_sample ../dt', <best_num> ,'/test_sample ../dt', <best_num> ,'/split300*.snp ../dt', <best_num> ,'/benchmark*.snp sample_feature/'))
    
    # make a directory to store all intermediate files (i.e. the files not have prediction result)
    system('mkdir inter_file')
    
    # # make a directory to store sh files for the pipeline from ADMIXTURE calculation to test set prediction for each running case
    # system('mkdir sh_file')
    
    # make a directory to store data frame with predicted result added in rdata format)
    system('mkdir prediction')
    
    # create plain text file to store prediction results and mean R2
    system('touch baseline_rf_rlt')
    system('touch bench_rf_rlt')
    system('touch split300_rf_rlt')
    system('touch mean_R2')

  ```
  
  Since this procedure is super time consuming, we highly suggest to run cases (having one sample as test set and the rest as training set) in parallel to save run time if users have access to any super computer. Here we show the pipeline to gather 7 cases as a job to run in parallel. The job submission for each job, requiring the number of nodes and running time, is ignored here.
  
  For each case, if the sample was in the training set of the best split set, AIM sets selected by test set of the best split set should be applied for this case, and vice versa.
  
  ```r
    reference_sample <- as.data.frame(read.table('sample_feature/reference_sample', header = T)[,c(1:2)])
    test_sample <- as.data.frame(read.table('sample_feature/test_sample', header = T)[,c(1:2)])
    
    # add GRC and Populations 
    reference_sample <- cbind(reference_sample, reference_sample[,1])
    colnames(reference_sample) <- c('Populations', 'GRC', 'sampleID')
    test_sample <- cbind(test_sample, test_sample[,1])
    colnames(test_sample) <- c('Populations', 'GRC', 'sampleID')
    
    
    # Add meta information
    meta <- read.csv('~/Data/meta_table') 
    reference_sample_meta <- add_meta_reich(reference_sample, meta)
    save(reference_sample_meta, file = 'reference_sample_meta.rdata')
    test_sample_meta <- add_meta_reich(test_sample, meta)
    save(test_sample_meta, file = 'test_sample_meta.rdata')
    
    
    ##### for test sample from the best split set #####
    index_lst_test <- c()
    ct <-1
    for(i in 1:nrow(test_sample_meta)){
      # use features selected from reference samples
      test_name <- paste0(test_sample_meta$Populations[i],test_sample_meta$sampleID[i])
      
      ### training sample ###
      the_training_samples_meta <- rbind(test_sample_meta[-i,], reference_sample_meta)
      training_sample_file_name <- paste0('inter_file/',test_name,'_training_sample')
      write.table(the_training_samples_meta[, c(3,2)], file = training_sample_file_name, 
                  quote = F, row.names = F, col.names = F)
      
      # extract training samples
      system(paste0('plink --bfile ../reich_here_overlap_qc2 --keep ',training_sample_file_name,
                    ' --make-bed --out inter_file/',test_name,'_training --noweb'))
      
      # merge genepool to training samples
      # for baseline
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out inter_file/',test_name,'_training_baseline_overlap --noweb --allow-no-sex'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_baseline_overlap.fam > inter_file/",test_name,"_training_baseline_overlap.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_baseline_overlap.pop"))
      
      # for benchmark
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training_baseline_overlap --extract sample_feature/benchmark.snp  --make-bed --out inter_file/',test_name,'_training_selected_bench --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_selected_bench.fam > inter_file/",test_name,"_training_selected_bench.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_selected_bench.pop"))
      
      # for split300
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training_baseline_overlap --extract sample_feature/socres_df.split.top300.snp  --make-bed --out inter_file/',test_name,'_training_selected_split300 --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_selected_split300.fam > inter_file/",test_name,"_training_selected_split300.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_selected_split300.pop"))
      
      
      ### test sample ###
      test_sample_file_name <- paste0('inter_file/',test_name,'_test_sample')
      write.table(test_sample_meta[i, c(3,2)], file = test_sample_file_name, 
                  quote = F, row.names = F, col.names = F)
      # extract test samples
      system(paste0('plink --bfile ../reich_here_overlap_qc2 --keep ',test_sample_file_name,
                    ' --make-bed --out inter_file/',test_name,'_test --noweb'))
      
      # merge genepool to test samples
      # for baseline
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out inter_file/',test_name,'_test_baseline_overlap --noweb --allow-no-sex'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_baseline_overlap.fam > inter_file/",test_name,"_test_baseline_overlap.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_baseline_overlap.pop"))
      
      # for benchmark
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test_baseline_overlap --extract sample_feature/benchmark.snp  --make-bed --out inter_file/',test_name,'_test_selected_bench --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_selected_bench.fam > inter_file/",test_name,"_test_selected_bench.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_selected_bench.pop"))
      
      # for split300
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test_baseline_overlap --extract sample_feature/socres_df.split.top300.snp  --make-bed --out inter_file/',test_name,'_test_selected_split300 --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_selected_split300.fam > inter_file/",test_name,"_test_selected_split300.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_selected_split300.pop"))
      
      
      index_lst_test <- c(index_lst_test, test_name)
      
      if(length(index_lst_test) == 7 | i == nrow(test_sample_meta)){ # one job submission for 7 cases
        message(paste0('=======================',paste(index_lst_test, collapse = ', '),'======================='))
        system(paste0("echo '# run R script\nRscript --vanilla loo_pipeline.R ",ct," ",paste(index_lst_test, collapse = ' '),
                      "' > sh_file/run_pipeline_",ct,".sh")) 
        system(paste0("sh sh_file/run_pipeline_",ct,".sh"))
        index_lst_test <- c()
        if(ct %% 10 == 0){ # to avoid the case that required nodes from running jobs exceed the number of nodes that can be accessed on the supercomputer, as well as to avoid full disk memory since too many files generated from jobs running in parallel
          Sys.sleep(600)
        }
        ct <- ct+1
      }
    }
    
    ##### for reference sample from the best split set #####
    index_lst <- c()
    for(i in 1:nrow(reference_sample_meta)){
      ### use features selected from test samples
      test_name <- paste0(reference_sample_meta$Populations[i],reference_sample_meta$sampleID[i])
      
      ### training sample ###
      the_training_samples_meta <- rbind(reference_sample_meta[-i,], test_sample_meta)
      training_sample_file_name <- paste0('inter_file/',test_name,'_training_sample')
      write.table(the_training_samples_meta[, c(3,2)], file = training_sample_file_name,
                  quote = F, row.names = F, col.names = F)
      
      # extract training samples
      system(paste0('plink --bfile ../reich_here_overlap_qc2 --keep ',training_sample_file_name,
                    ' --make-bed --out inter_file/',test_name,'_training --noweb'))
      # merge genepool to training samples
      # for baseline
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out inter_file/',test_name,'_training_baseline_overlap --noweb --allow-no-sex'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_baseline_overlap.fam > inter_file/",test_name,"_training_baseline_overlap.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_baseline_overlap.pop"))
      
      # for benchmark
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training_baseline_overlap --extract sample_feature/benchmark_test.snp  --make-bed --out inter_file/',test_name,'_training_selected_bench --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_selected_bench.fam > inter_file/",test_name,"_training_selected_bench.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_selected_bench.pop"))
      
      # for split300
      system(paste0('plink --bfile inter_file/',test_name,
                    '_training_baseline_overlap --extract sample_feature/socres_df.split.top300_test.snp  --make-bed --out inter_file/',test_name,'_training_selected_split300 --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_training_selected_split300.fam > inter_file/",test_name,"_training_selected_split300.pop.txt"))
      system(paste0("printf '%.0s\n' {1..",nrow(the_training_samples_meta),"}  > inter_file/",test_name,"_training_selected_split300.pop"))
      
      ### test sample ###
      test_sample_file_name <- paste0('inter_file/',test_name,'_test_sample')
      write.table(reference_sample_meta[i, c(3,2)], file = test_sample_file_name,
                  quote = F, row.names = F, col.names = F)
      # extract test samples
      system(paste0('plink --bfile ../reich_here_overlap_qc2 --keep ',test_sample_file_name,
                    ' --make-bed --out inter_file/',test_name,'_test --noweb'))
      # merge genepool to test samples
      # for baseline
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test --bmerge ../genepool_overlap_qc.bed ../genepool_overlap_qc.bim ../genepool_overlap_qc.fam  --make-bed --out inter_file/',test_name,'_test_baseline_overlap --noweb --allow-no-sex'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_baseline_overlap.fam > inter_file/",test_name,"_test_baseline_overlap.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_baseline_overlap.pop"))
      
      # for benchmark
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test_baseline_overlap --extract sample_feature/benchmark_test.snp  --make-bed --out inter_file/',test_name,'_test_selected_bench --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_selected_bench.fam > inter_file/",test_name,"_test_selected_bench.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_selected_bench.pop"))
      
      # for split300
      system(paste0('plink --bfile inter_file/',test_name,
                    '_test_baseline_overlap --extract sample_feature/socres_df.split.top300_test.snp  --make-bed --out inter_file/',test_name,'_test_selected_split300 --noweb'))
      system(paste0("cut -f1-2 -d ' ' inter_file/",test_name,"_test_selected_split300.fam > inter_file/",test_name,"_test_selected_split300.pop.txt"))
      system(paste0("printf '%.0s\n' {1}  > inter_file/",test_name,"_test_selected_split300.pop"))
      
      index_lst <- c(index_lst, test_name)
      
      if(length(index_lst) == 7 | i == nrow(reference_sample_meta)){ # one job submission for 7 cases
        message(paste0('=======================',paste(index_lst_test, collapse = ', '),'======================='))
        system(paste0("echo '# run R script\nRscript --vanilla loo_pipeline.R ",ct," ",paste(index_lst_test, collapse = ' '),
                      "' > sh_file/run_pipeline_",ct,".sh")) 
        system(paste0("sh sh_file/run_pipeline_",ct,".sh"))
        index_lst <- c()
        if(ct %% 10 == 0){# to avoid the case that required nodes from running jobs exceed the number of nodes that can be accessed on the supercomputer, as well as to avoid full disk memory since too many files generated from jobs running in parallel
          Sys.sleep(600)
        }
        ct <- ct+1
      }
    }

  ```

  where *loo_pipeline.R*:
  ```r
    args <- commandArgs(trailingOnly=TRUE)
    ct <- args[[1]]
    samples <- args[-1]
    message(paste0('rf_',ct,'.sh is generated for ',paste(samples, collapse = ', ')))
    
    system(paste0("echo '# run R script\n' > sh_file/rf_",ct,".sh"))    
    for(ind in samples){ # 7 samples
      test_name <- ind
      
      ### extract AIM sets + ADMIXTURE in supervised mode for both training set and test set 
      ### training sample ###
      system(paste0('sh feature_selection.sh inter_file/ ',test_name,'_training '))
      
      ### test sample ###
      system(paste0('sh feature_selection.sh inter_file/ ',test_name,'_test'))
      
      ### write into sh file and remove intermediate files
      system(paste0("echo 'Rscript --vanilla run_rf_LOO.R ",test_name,"' >> sh_file/rf_",ct,".sh"))
      system(paste0('rm inter_file/',test_name,'*')) # to avoid full disk memory
      
    }
    system(paste0("sh sh_file/rf_",ct,".sh"))    
          
  ```            
  
  where *feature_selection.sh*:
  ```console
    #!/bin/sh

    # $1: directory path (i.e. inter_file/)
    # $2: test_name (<Population><name> for each sample) 

    # baseline
    cat $1$2_baseline_overlap.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> $1$2_baseline_overlap.pop

    ~/admixture32 $1$2_baseline_overlap.bed -F 9 -j8
    mv $2_baseline_overlap* $1/
    cat $1$2_baseline_overlap.fam | cut -d ' ' -f1-2 > $1$2_out_ind_id
    sed -i 's/ /\t/g' $1$2_out_ind_id
    sed -i 's/ /\t/g' $1$2_baseline_overlap.9.Q
    paste $1$2_out_ind_id $1$2_baseline_overlap.9.Q > $1out_Q_$2_baseline
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  $1out_Q_$2_baseline

    # benchmark
    wc -l $1$2_selected_bench.pop

    cat $1$2_selected_bench.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> $1$2_selected_bench.pop
    wc -l $1$2_selected_bench.pop

    ~/admixture32 $1$2_selected_bench.bed -F 9 -j8
    mv $2_selected_bench* $1/
    cat $1$2_selected_bench.fam | cut -d ' ' -f1-2 > $1$2_out_ind_id_bench
    sed -i 's/ /\t/g' $1$2_out_ind_id_bench
    sed -i 's/ /\t/g' $1$2_selected_bench.9.Q
    paste $1$2_out_ind_id_bench $1$2_selected_bench.9.Q > $1out_Q_$2_bench
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  $1out_Q_$2_bench

    # split300
    cat $1$2_selected_split300.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> $1$2_selected_split300.pop

    ~/admixture32 $1$2_selected_split300.bed -F 9 -j8
    mv $2_selected_split300* $1/
    cat $1$2_selected_split300.fam | cut -d ' ' -f1-2 > $1$2_out_ind_id_split300
    sed -i 's/ /\t/g' $1$2_out_ind_id_split300
    sed -i 's/ /\t/g' $1$2_selected_split300.9.Q
    paste $1$2_out_ind_id_split300 $1$2_selected_split300.9.Q > $1out_Q_$2_split300
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  $1out_Q_$2_split300

    
  ```
      
  and *run_rf_LOO.R*:
  ```r
    args <- commandArgs(trailingOnly=TRUE)
    iteration <- c('baseline', 'bench', 'split300')
    ind <- args[[1]]
    
    for(ite in iteration){ # basleine; benchmark; split300
      message(paste('==========',ind,ite,'=========='))
      
      qfile_train <- read.table(paste0('inter_file/out_Q_',ind,'_training_',ite), header = T, sep = '\t') 
      qfile_test <- read.table(paste0('inter_file/out_Q_',ind,'_test_',ite), header = T, sep = '\t')
      
      # Add meta information
      meta <- read.csv('~/Data/meta_table')
    
      qfile_train_nogp <- qfile_train[-which(qfile_train$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                           'SouthAfrican', 'SouthWestAsian',
                                                                           'NativeAmerican', 'Oceanian',
                                                                           'SouthEastAsian', 'NorthernEuropean',
                                                                           'SubsaharanAfrican')), ] 
      qfile_train_nogp$Populations<- as.character(qfile_train_nogp$Populations)
      qfile_train_nogp_popFilter <- add_meta_reich(qfile_train_nogp, meta)
      qfile_train_nogp_popFilter <- droplevels(qfile_train_nogp_popFilter)
      print('qfile_train_nogp_popFilter:')
      str(qfile_train_nogp_popFilter)
      
      qfile_test_nogp <- qfile_test[-which(qfile_test$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                                        'SouthAfrican', 'SouthWestAsian',
                                                                        'NativeAmerican', 'Oceanian',
                                                                        'SouthEastAsian', 'NorthernEuropean',
                                                                        'SubsaharanAfrican')), ] 
      qfile_test_nogp$Populations<- as.character(qfile_test_nogp$Populations)
      qfile_test_nogp_popFilter <- add_meta_reich(qfile_test_nogp, meta)
      qfile_test_nogp_popFilter <- droplevels(qfile_test_nogp_popFilter)
      print('qfile_test_nogp_popFilter:')
      str(qfile_test_nogp_popFilter)
      
      qfile_train_nogp_popFilter$GRC <- as.character(qfile_train_nogp_popFilter$GRC)
      if(sum(is.na(qfile_train_nogp_popFilter$longitude)) > 0){
        qfile_train_nogp_popFilter <- qfile_train_nogp_popFilter[-which(is.na(qfile_train_nogp_popFilter$longitude)),]
      }
      qfile_test_nogp_popFilter$GRC <- as.character(qfile_test_nogp_popFilter$GRC)
      if(sum(is.na(qfile_test_nogp_popFilter$longitude)) > 0){
        qfile_test_nogp_popFilter <- qfile_test_nogp_popFilter[-which(is.na(qfile_test_nogp_popFilter$longitude)),]
      }
      if(nrow(qfile_test_nogp_popFilter) != 1){
        warning(paste0('Test sample ',ind,' does not have valid longitude, skip run RF'))
      }else{
        save(qfile_test_nogp_popFilter, file = paste0('prediction/',ind,'_',ite,'_rf.rdata'))
        
        ### Model training ###
        rf_model_training_train_test(qfile_train_nogp_popFilter, qfile_test_nogp_popFilter, tag = c(ind, ite))
        
      }
    }
      
  ```
  After prediction of all cases have been done,  combine them into one data frame
  
  To have the prediciton result file list to iterate, get the output rdata file list in Bash command:
  ```console
     ls prediction/ > prediction_file_names                                
  ``` 
  
  Iteratively bind all predictions into one data frame in R 4.1.2.
  ```r
    ## load file name list
    file_names <- read.table('prediction_file_names', sep = '\t', header = F)[,1]
    
    ## prepare data frames
    baseline_qfile <- data.frame()
    bench_qfile <- data.frame()
    split300_qfile <- data.frame()
    unknown <- data.frame()
    prediction_names <- c()
    unkown_prediction_names <-c()
    
    ## Iterate all file names, load the file, and classify the data
    for(the_file_name in file_names){
      load(paste0('prediction/',the_file_name))
      
      file_name_split <- strsplit(gsub('.rdata','',the_file_name), 
                                  split = '_')[[1]]
      if(file_name_split[[(length(file_name_split)-1)]] == 'baseline'){ # classify to baseline
        if(file_name_split[[length(file_name_split)]] == 'qfile'){
          if(nrow(baseline_qfile) < 1){
            baseline_qfile <- as.data.frame(add_preds)
          }else{
            baseline_qfile <- rbind(baseline_qfile, add_preds)
          }
          prediction_names <- c(prediction_names, paste0(add_preds[i,1], add_preds[i,2]))
        }
      }else if(file_name_split[[(length(file_name_split)-1)]] == 'bench'){ # classify to benchmark
        if(file_name_split[[length(file_name_split)]] == 'qfile'){
          if(nrow(bench_qfile) < 1){
            bench_qfile <- as.data.frame(add_preds)
          }else{
            bench_qfile <- rbind(bench_qfile, add_preds)
          }
          
        }
      }else if(file_name_split[[(length(file_name_split)-1)]] == 'split300'){ # classify to split300
        if(file_name_split[[length(file_name_split)]] == 'qfile'){
          if(nrow(split300_qfile) < 1){
            split300_qfile <- as.data.frame(add_preds)
          }else{
            split300_qfile <- rbind(split300_qfile, add_preds)
          }
          
        }
      }else{ # in case any unrecognized file name
        if(file_name_split[[length(file_name_split)]] == 'qfile'){
          if(nrow(unknown) < 1){
            unknown <- as.data.frame(add_preds)
          }else{
            unknown <- rbind(unknown, add_preds)
          }
          unkown_prediction_names <-c(unkown_prediction_names,  file_name_split[[1]])
        }
        
      }
      
    }
    
    ## Check if there is any unrecognized prediction result
    unkown_prediction_names <- unique(unkown_prediction_names)
    if(length(unkown_prediction_names) >= 1){
      print('unknown prediciton names: ')
      print(unkown_prediction_names)
      
    }else{
      print('No unknown prediction file!')
      message('No unknown prediction file!')
      
    }
    
    ## Save combined prediction results into rdata file
    save(baseline_qfile, file = '~/Data/baseline_qfile.rdata')
    save(bench_qfile, file = '~/Data/bench_qfile.rdata')
    save(split300_qfile, file = '~/Data/split300_qfile.rdata')

  ```
  
> 4. Visualization
  All codes for visualiation below can be applied to the data frame with predicted geographic coordinates added. Therefore, the codes below are sample code for each plot 
  ```r
    ##### ADMIXTURE Q file bar plot #####
    # Load data frame either by load or by read.csv. 
    # Note that ancestral population portion for each geographic coordinate, geographic coordinate, and country name must be contained
    load('<data_frame_name>.rdata') 
    
    # Convert the data name to its AIM set name (baseline/bench/split300)
    baseline <- <name of loaded data frame> # assume a baseline result is loaded
    # Reorder the columns so that
    # The order of columns for the bar plot:
      # Populations, GRC, 
      # North East Asian, Mediterranean, South African,
      # South West Asian, Native American, Oceanian, 
      # South East Asian, Northern European, Subsaharan African,
      # country, latitude, longitude
    baseline_nogp.order <- baseline[,c(1,2,5,3,8,10,4,7,9,6,11,12:14)]
    
    barplot_admixture(baseline_nogp.order, 
                      pic_file_path = '<bar plot file path>.pdf')
    
    #####distance from the origin in level bar plot #####
    # Load data frame either by load or by read.csv. 
    # Note that ancestral population portion for each geographic coordinate, geographic coordinate, predicted geographic coordinate, and country name must be contained

    load('<data_frame_name>.rdata') 
    
    # convert the name of loaded data frame
    MetasubDataPreds <- <name of loaded data frame>
    for (i in 1:nrow(MetasubDataPreds)){
      MetasubDataPreds[i,"Distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
    }
    
    png_path  <- paste0('<bar plot file path>.png')
    distance_diff_plot_ctry(MetasubDataPreds, png_path) 
    
    #####map plot ####
    # Load data frame either by load or by read.csv. 
    # Note that ancestral population portion for each geographic coordinate, geographic coordinate, predicted geographic coordinate, and country name must be contained

    load('<data_frame_name>.rdata') 
    
    # convert the name of loaded data frame
    MetasubDataPreds <- <name of loaded data frame>
    
    # get continent name based on corresponding country  
    MetasubDataPreds$continent <- countrycode::countrycode(sourcevar = MetasubDataPreds$country,
                                                           origin = "country.name",
                                                           destination = "continent")
    
    # Manually assign unknown continent
    # If these 2 countries do not present in the data frame, just skip
    MetasubDataPreds$continent[which(MetasubDataPreds$country=='Abkhazia')] <- 'Asia'
    MetasubDataPreds$continent[which(MetasubDataPreds$country=='Czechoslovakia')] <- 'Europe'
    
    distance_map_visualization(MetasubDataPreds, filter_level='all', png_path = '<file path without pic format>') # filter_level will be added as suffix into the final file path
    distance_map_visualization(MetasubDataPreds, filter_level='notSameContinent', png_path = '<file path without pic format>') 
    distance_map_visualization(MetasubDataPreds, filter_level='notSameContinent1000', png_path = '<file path without pic format>')
    distance_map_visualization(MetasubDataPreds, filter_level='notSameContinent500', png_path = '<file path without pic format>')


  ```
 
  
                


