# master degree project: a forensic grographic corrdinate prediction pipeline
#### course: degree project
#### credit: 45
#### student: Yuexin Yu
#### Supervisor: Eran Elhaik

This is the README file containing the workflow to construct this forensic geopragic cooridnate prediction pipeline.

The required input are available in *Data* directory.

All required packages in R are listed in packages.R

> Introduction. 

  This is an admixture-based pipeline to predict smaplesâ€˜ geographic coordinates in given test set. The pipeline applies the following general workflow:  
  * 1. Extract an AIM set from both training set and test set
  * 2. Calculate portions of pupative ancestral populations of traing set and test set repsectively, by ADMIXTURE in supervised mode.
  * 3. Predict the geographic coordinates for samples in test set based on the model trained by Random Forest using training set.   

> Load packages in R 4.1.2
``` r

```

  
> 0. Functions
```r
########## rf_latlong ##########
rf_latlong <- function(training, testing, variables, coast=NULL, nthread=8){
  # Model training by random forest method and use the model to predict test set. Then adjust predicted geographic coordinates based on world point data and coastline data
  # @training: the data frame of training set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @test: the data frame of test set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @variables: a list of column names used in model training
  # @coast: If coast = NULL, which is the feault, it means no need data adjustment after prediction; otherwise, it should be the coastline data generated in rf_model_training function. 
  # @nthread: number of threads to use when running this function. The default is 8.
  
  set.seed(1234)
  
  # create 5 subsets in training for cross validation
  folds <- caret::createFolds(training[,'Populations'], k = 5, returnTrain = T)
  
  # ask cross validation when model training; the model training will be a regression
  trControl <-  trainControl( # regression
    method = "cv",
    number = 5,
    verboseIter = FALSE,
    returnData = FALSE,
    search = "grid",
    savePredictions = "final",
    allowParallel = T,
    index = folds)
  
  # use a range of value as the value of parameter so that all value will be tested and the model with smallest RMSE will be finally used
  tune_grid <- expand.grid(.mtry = (1:15))
  
  # train for latitude
  training$rowIndex <- as.numeric(rownames(training))
  Xgb_latitude <- train(x = training[,variables],
                        y = training[,'latitude'],
                        method = "rf",
                        trControl = trControl,
                        tuneGrid = tune_grid ,
                        nthread = nthread
  )

  # train for longitude
  Xgb_longitude <- train(x = training[,variables],
                         y = training[,'longitude'],
                         method = "rf",
                         trControl = trControl,
                         tuneGrid = tune_grid ,
                         nthread = nthread
  )
  
  message(paste('the mean R2 for current model',
              mean(Xgb_longitude$results$Rsquared[which(Xgb_longitude$results$mtry == Xgb_longitude$bestTune[[1]])],
               Xgb_latitude$results$Rsquared[which(Xgb_latitude$results$mtry == Xgb_latitude$bestTune[[1]])])), 
          )
  
  
  latPred <- predict(Xgb_latitude, newdata = testing[,variables])
  longPred <- predict(Xgb_longitude, newdata = testing[,variables])
  
  #adjust out of bounds predictions
  message('adjust out of bounds predictions')
  longPred[longPred > 180] <- 180
  longPred[longPred < -180] <- -180
  latPred[latPred > 90] <- 90
  latPred[latPred < -90] <- -90
  #Pull to nearest coastline if provided
  # message('Pull to nearest coastline if provided')
  find_coast <- function(long, lat) { # find the closet point on the coast for the given long and lat
    distances_from_coastline <-
      sp::spDistsN1(coast, c(long, lat), longlat = TRUE)
    
    closest_point <-  which.min(distances_from_coastline)
    new_coords <- coast[closest_point,]
    
    return(new_coords)
  }
  
  if (!is.null(coast)) {
    message('toAdjust generated by function map.where')
    #find the longPred / latPred that are not on world latitude/longitude
    toAdjust <- which(is.na(maps::map.where(database = "world", longPred, latPred)))
    
    if(length(toAdjust) > 0){
      # apply find_coast function to adjust the latitude and longitude of given toAdjust index
      message('adjusted generated by mapply find_coast and longPred, latPred')
      adjusted <- mapply(find_coast, long = longPred[toAdjust], lat = latPred[toAdjust])
      
      # update the adjusted lat and long
      longPred[toAdjust] <- adjusted[1,]
      latPred[toAdjust] <- adjusted[2,]
    }
    
    
    
  }
  
  
  message('return')
  return(list(latPred, longPred))
  
  
}





########## rf_model_training ##########
rf_model_training <- function(qfile_nogp_popFilter, tag, extraColumn=''){
  
  # @qfile_nogp_popFilter: the data frame with sample names, corresponding ancestral population portion calculations, sample's latitude and longitude, GRC, as well as country
  # @tag: a string which is the suffix added in the file path 
  # @extraColumn: a string. If run model training in random forest method, extraColumn = '', which is the default. If run model training in random forest with an extra column added, extraColumn = <the extra column name>
  
  # prepare coastline data for the adjustment of predicted geographic coordinate
  coastlines <- cbind("x"  = maps::SpatialLines2map(rworldmap::coastsCoarse)$x ,"y" =maps::SpatialLines2map(rworldmap::coastsCoarse)$y)
  coastlines <- coastlines[complete.cases(coastlines),]
  coastlines <- coastlines[coastlines[,1] < 180 ,]
  
  GeoPreds <- list()
  
  # extract the ancestral population names
  gp <- colnames(qfile_nogp_popFilter)[-c(1,2,12,13,14)]
  
  # add the name of extra column if needed
  if(extraColumn != ''){
    gp <- c(gp, extraColumn)
    
  }
  
  set.seed(18)
  
  # split qfile_nogp_popFilter into 5 subsets.
  trainFolds <-  caret::createFolds(qfile_nogp_popFilter$Populations, k = 5, returnTrain = T)
  
  GeoPreds <- list()
  
  # For each iteration, use 4 subsets as training set, the left one as test set to do model training and prediction. Since there are 5 subsets, iteration is 5 so that all samples are finally tested
  start_time <- Sys.time()
  for (i in 1:5){
    
    train <- qfile_nogp_popFilter[trainFolds[[i]],]
    test <- qfile_nogp_popFilter[-trainFolds[[i]],]
    
    start_time.adm <- Sys.time()
    testPreds <-rf_latlong(training = train, testing = test,
                           variables = gp, coast=coastlines)
    GeoPreds[[i]] <- testPreds
    
    end_time.adm <- Sys.time()
    time_for_testPreds <- end_time.adm - start_time.adm
    # message(time_for_testPreds)
    
    
  }
  end_time <- Sys.time()
  time_for_testPreds_ind <- end_time - start_time
  message(time_for_testPreds_ind) # 
  
  # Add predicted geographic coordinates to corresponding samples
  add_preds <- list()
  for (i in 1:5){
    
    add_preds[[i]] <- cbind(qfile_nogp_popFilter[-trainFolds[[i]],] ,
                            "latPred" = GeoPreds[[i]][[1]],
                            "longPred" = GeoPreds[[i]][[2]] )
    
  }
  
  # Add predicted geographic coordinates to corresponding samples
  MetasubDataPreds <- plyr::rbind.fill(add_preds)
  
  # Save the data frame as a csv file
  if(extraColumn == ''){
    write.csv(MetasubDataPreds,paste0("qfile_predict_rf_",tag,".csv"))
    
  }else{
    write.csv(MetasubDataPreds,paste0("qfile_predict_extraCol_",tag,".csv"))
    
  }
  
  # MetasubDataPreds <- read.csv(paste0("qfile_predict_rf_",tag,".csv"))
  
  # Calculate the distance between the original and the predicted geographic coordinates, added to column 'Distance_from_origin'
  for (i in 1:nrow(MetasubDataPreds)){
    MetasubDataPreds[i,"Distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
  }
  
  # Print distance from origin results 
  print(c(mean(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin[which(MetasubDataPreds$Distance_from_origin < 100)])))
}


maf_tbl_generation <- function(marker_frq, marker_ped,
                               rdata_output_path){
  # Return a table having sample in row, SNP in column. Each cell is the minor allele frequency of the SNP in current column for the sample in current row.
  
  # @marker_frq: The data frame read from .frq file
  # @marker_ped: The data frame read from .ped file
  # @rdata_output_path: A strin gof file path to save the MAF data frame in rdata
  
  marker_frq <- marker_frq[-1,]
  minor_allele.df <- data.frame(SNP = marker_frq$V2, MA = marker_frq$V3)
  
  marker_ped.allele <-  marker_ped[,c(1,7:ncol(marker_ped))] # row: individual column: genotype (biallelic)
  
  print('Start MAF')
  
  # Minor allele frequencies calculation
  start_time <- Sys.time()
  pop <- unique(marker_ped.allele$V1)
  if(length(pop) > 1){
    marker_freq <- data.frame(population = pop) # note that iteration of marker_ped.allele$V1 = marker_freq$individual
    for(i in 1:nrow(minor_allele.df)){ # i is index of marker
      if(i %% 10000 == 0){ # easy to track
        message(paste('Currently in i=', i))
      }
      freqs <- c()
      minor_allele <- minor_allele.df[i,2]
      for(j in pop){ #j is individual
        pop_ave <- mean(stringr::str_count(marker_ped.allele[which(marker_ped.allele$V1 %in% j),1+i],
                                           pattern = as.character(minor_allele))
                        * 0.5 ) # get the average of minor allele frequency of one population for current marker
        freqs <- c(freqs, pop_ave)
      }
      marker_freq <- cbind(marker_freq, freqs) # bind the minor allele frequencies for all individual in this marker to the result data frame
    }
  }else{# length(pop) = 1
    marker_freq <- data.frame(population = marker_ped.allele$V1) # note that iteration of marker_ped.allele$V1 = marker_freq$individual
    for(i in 1:nrow(minor_allele.df)){ # i is index of marker
      if(i %% 10000 == 0){ # easy to track
        message(paste('Currently in i=', i))
      }
      freqs <- c()
      minor_allele <- minor_allele.df[i,2]
      for(j in 1:nrow(marker_ped.allele)){ #j is individual
        # print(paste('j: ', j))
        pop_ave <- mean(stringr::str_count(marker_ped.allele[j,1+i],
                                           pattern = as.character(minor_allele))
                        * 0.5 ) # get the average of minor allele frequency of one population for current marker
        freqs <- c(freqs, pop_ave)
        
      }
      marker_freq <- cbind(marker_freq, freqs) # bind the minor allele frequencies for current individual in this marker to the result data frame
      
    }
  }
  
  colnames(marker_freq) <- c('Populations', minor_allele.df$SNP) # SNP name as column name
  
  end_time <- Sys.time()
  time_for_marker_freq <- end_time - start_time
  print(time_for_marker_freq)
  
  # Save MAF table
  metasub_data<- marker_freq
  save(metasub_data, file = rdata_output_path)
  return(metasub_data)
}


add_meta_ref <- function(metasub_data, meta){
  # Return metasub_data with corresponding country, latitude, longitude added for all samples
  # This function is only used for output_645 set

  # @metasub_data: A data frame having population and GRC information for all samples from output_645 set
  # @meta: A data frame having meta data for output_645 set
  
  # Remove meta data not having enough information
  meta_latlong <- meta[which(sapply(X = meta$inCountry, FUN = isTRUE)),]
  
  # Remove samples having population name that not in the meta data
  metasub_data <- metasub_data[which(metasub_data$Populations %in% meta_latlong$nameArgument),]
  meta_latlong$country <- make.names(meta_latlong$country)
  
  # Add country, latitude, longitude to the corresponding population
  metasub_data$country <- NA
  metasub_data$latitude <- NA
  metasub_data$longitude <- NA
  for(i in 1:nrow(metasub_data)){
    the_population <- metasub_data$Populations[i]
    metasub_data$country[i] <- meta_latlong$country[which(meta_latlong$nameArgument == the_population)]
    metasub_data$latitude[i] <- meta_latlong$latitidue[which(meta_latlong$nameArgument == the_population)]
    metasub_data$longitude[i] <- meta_latlong$longitude[which(meta_latlong$nameArgument == the_population)]
  }
  
  return(metasub_data)
}


add_meta_reich <- function(qfile_nogp, meta){
  # Return qfile_nogp with meta data (country, latitude, longitude) added, and population is also changed to corresponding version ID in AADR set
  # This function is only used for AADR set
  
  # @qfile_nogp: A data frame having population and GRC information for all samples from AADR set
  # @meta: A data frame having meta data for AADR set
  
  # filter out meta data not in given data frame, and the samples from the given data frame that not in meta data
  meta_pop <- meta[which(meta$Version.ID %in% qfile_nogp$GRC),]
  message(nrow(meta_pop))
  qfile_nogp_popFilter <- qfile_nogp[which(qfile_nogp$GRC %in% meta_pop$Version.ID),]
  message(nrow(qfile_nogp_popFilter)) # the number of rows in the given data frame and meta data should be the same after filtration
  
  meta_pop$Country <- make.names(meta_pop$Country)
  meta_pop$Version.ID <- as.character(meta_pop$Version.ID)
  
  # Add country, latitude, longitude to the corresponding population
  qfile_nogp_popFilter$country <- NA
  qfile_nogp_popFilter$latitude <- NA
  qfile_nogp_popFilter$longitude <- NA
  for(i in 1:nrow(qfile_nogp_popFilter)){
    versionID <- qfile_nogp_popFilter$GRC[i]
    qfile_nogp_popFilter$Populations[i] <- as.character(meta_pop$Group_Label[which(meta_pop$Version.ID == versionID)])
    qfile_nogp_popFilter$country[i] <- as.character(meta_pop$Country[which(meta_pop$Version.ID == versionID)])
    qfile_nogp_popFilter$latitude[i] <- as.numeric(as.character(meta_pop$Lat.[which(meta_pop$Version.ID == versionID)]))
    qfile_nogp_popFilter$longitude[i] <- as.numeric(as.character(meta_pop$Long.[which(meta_pop$Version.ID == versionID)]))
  }
  
  return(qfile_nogp_popFilter)
}


null_importance_select <-function(x, features, seed=123, shuffle=F,cores = 16) {
  # Return a feature importance table for all feature by fit model to latitude and longitude respectively, and sum the feature importance of both latitude and longitude as one feature's feature importance.
  
  # @x: A data frame containing columns used for model training, and the target to fit
  # @features: A list of feature names in x, which is used in model training
  # @shuffle: A boolean. If FALSE, the targets map to corresponding sample, which is the default; otherwise, the targets will be randomly shift to other samples
  # @cores: An integer. The number of cores to run this function
  
  
  y.lat <- x$latitude
  y.long <-x$longitude
  
  # Shuffle target if required
  if(isTRUE(shuffle)){
    
    new_order_x <- x[sample(nrow(x), size = nrow(x)),]
    y.lat <- new_order_x$latitude
    y.long <- new_order_x$longitude
  }
  
  dtrain.lat <- lgb.Dataset(data = as.matrix(x[,-c(ncol(x)-1,ncol(x))]), label = y.lat, free_raw_data=F)
  
  lgb_params <- list(objective = "regression",
                     boosting = 'rf',
                     subsample = 0.623,
                     colsample_bytree = 0.7,
                     num_leaves = 137,
                     max_depth = 8,
                     seed = seed,
                     bagging_freq = 1,
                     n_jobs = cores)
  
  # fit the model for latitude
  clf.lat <- lgb.train(params=lgb_params, data=dtrain.lat, nrounds=200, init_model=NULL)
  
  
  
  dtrain.long <- lgb.Dataset(data = as.matrix(x[,-c(ncol(x)-1,ncol(x))]), label = y.long, free_raw_data=F)
  
  # fit the model for longitude
  clf.long <- lgb.train(params=lgb_params, data=dtrain.long, nrounds=200)
  
  # Get feature importances, sum the feature importance in latitude and longitude as a feature's feature importance
  ftr_importance.long <- lgb.importance(clf.long, percentage = F)[,c('Feature','Gain', 'Frequency')]
  ftr_importance.lat <- lgb.importance(clf.lat, percentage = F)[,c('Feature','Gain', 'Frequency')]
  for(i in 1:nrow(ftr_importance.long)){
    current_fea <- ftr_importance.long$Feature[i]
    if(current_fea %in% ftr_importance.lat$Feature){ # lat + long
      ftr_importance.lat$Gain[which(ftr_importance.lat$Feature == current_fea)] <- sum(c(ftr_importance.lat$Gain[which(ftr_importance.lat$Feature == current_fea)], ftr_importance.long$Gain[i]))
      ftr_importance.lat$Frequency[which(ftr_importance.lat$Feature == current_fea)] <- sum(c(ftr_importance.lat$Frequency[which(ftr_importance.lat$Feature == current_fea)], ftr_importance.long$Frequency[i]))
    }else{
      ftr_importance.lat <- rbind(ftr_importance.lat, ftr_importance.long[i,])
    }
  }
  imp_df <- ftr_importance.lat
  colnames(imp_df) <- c('feature', 'importance_gain', 'importance_split')
  
  return(imp_df)
}





```
> 1. AADR set data preparation
  Codes here are perfomed in Bash command. [PLINK 1.9](https://www.cog-genomics.org/plink/1.9/) is required. Since some of file sizes here are over the size limit on github, only the result files with the prefix *reich_here_overlap* are provided
  ```console
  # Get overlaps between ancestrial population set and AADR set
  plink --bfile ../Genographic/num_Admixture_reference_pops --extract reich_here.bim --make-bed --out genepool_overlap_SNP --noweb
  
  # keep only overlapped SNPs in AADR set
  plink --bfile reich_here --extract ../Genographic/num_Admixture_reference_pops.bim --make-bed --out reich_here_overlap --noweb
  ```
  
> 2. AIM set curation using AADR set 

Note: Only the final result files (in 2..) is provided in *Data* directory

* 2.1. Randomly split AADR samples into 2 based on a filtration criteria, do 100 times

The filtration criteria:  

Before data splitting, countires having sample size < 5 should be discarded.  

During splitting, split smaples in each country into 2 sets in same size. If the sample size after splitting < 5, all samples in this country are put into training set. Otherwise, training set and test set will get random samples from this country in same size.   

Codes here are performed in R 4.1.2. 

```r
  # load meta
  meta <- read.csv('Data/meta_table') #nrow(meta)=14008
  # load sample tbl
  fam <- read.table('Data/reich_here_overlap.fam')[,2] 
  fam_file <- read.table('Data/reich_here_overlap.fam')
  meta <- meta[which(meta$Version.ID %in% fam),] 

  # remove countries having samples < 5
  ctry_count <- as.data.frame(table(meta$Country))
  smaller_than_five <- ctry_count$Var1[which(ctry_count$Freq<=5)]
  meta <- meta[-which(meta$Country %in% smaller_than_five ),] 
  ctry_count <- ctry_count[-which(ctry_count$Freq<=5),] 


  for(j in 1:100){
  sample_set1 <- c()
  sample_set2 <- c()
  
  # select half of samples from each country
   for(i in 1:nrow(ctry_count)){
     ctry <- ctry_count$Var1[i]
     ctry_sample <- meta$Version.ID[which(meta$Country == ctry)]
     split_size <- ceiling(length(ctry_sample)/2)
     if(split_size >= 5){ # if sample size after splitting < 5, all samples in this country are put into training set
       sample_set1 <- c(sample_set1, sample(ctry_sample, size = split_size))
       sample_set2 <- c(sample_set2, ctry_sample[-which(ctry_sample %in% sample_set1)])
     }else{ # if not, put into 2 sets
       sample_set1 <- c(sample_set1, ctry_sample)
     }
   }
   if(length(sample_set1) + length(sample_set2) == nrow(meta)){ # ensure no sample missing
     reference_sample <- fam_file[which(fam_file$V2 %in% sample_set1),]
     test_sample <- fam_file[which(fam_file$V2 %in% sample_set2),]
      
     # save them into file, ans save to a corresponding directory
     dir.create(paste0('dt',j))
     write.table(reference_sample, file = paste0('dt',j,'/reference_sample'),
                  quote = F, row.names = F, sep = '\t')
     write.table(test_sample, file = paste0('dt',j,'/test_sample'),
                  quote = F, row.names = F, sep = '\t')
   }else{
     message(nrow(reference_sample))
     message(nrow(test_sample))
     stop(paste0('In iteration ',j,', the sum of reference sample size and test sample size is not equal to the size of Reich dataset'))
   }
  
  
  }

```

* 2.2. For each set of split AADR training and test sets:
  
All R code in section 2.2. can be in one R file *ref_pipeline.R*, and run with arugument passing through this script in Bash command such as:

(Note that *<num>* represents *the number of current set from 100 runs*)

```console
  Rscript --vanilla ref_pipeline.R <num>
```
  
In ref_pipeline.R, start with:
```r
  args <- commandArgs(trailingOnly=TRUE)
  ii <- args[1]
  setwd(paste0('dt',ii))
```
  
to navigate to the directory having split sample sets in the number of current set from 100 runs.

  + 2.2.1. Extract samples for training set and test set
  
  Codes here are performed in R. PLINK 1.9 is required. 
  ```r
     system('plink --bfile ../../reich_here_overlap --keep reference_sample --make-bed --out reference_reich --noweb')
     system('plink --bfile ../../reich_here_overlap --keep test_sample --make-bed --out test_reich --noweb')

  ```
  + 2.2.2. Prepare training set + run ADMIXTURE in supervised mode for training set
  
  Codes here are performed in R. Both [PLINK 1.07](https://zzz.bwh.harvard.edu/plink/download.shtml) and PLINK 1.9 are required.
  ```r
    system('sh ../baseline_preparation')
  ```
  
  where *baseline_preparation*:
  
  (Note that ADMIXTURE is required, and is prepared in main page named as *admixture32*)
  ```console
    # since the coding of base is different for AADR set and ancestral population set, convert the base in AADR set
    ~/bin/plink-1.07-x86_64/plink --bfile reference_reich --allele1234 --make-bed --out reference_reich_qc --noweb

    # try to merge training set with ancestral population set, will get an error due to different allelic location in 2 sets, will automatically generate a .missno file
    plink --bfile reference_reich_qc --bmerge ../../genepool_overlap.bed ../../genepool_overlap.bim ../../genepool_overlap.fam  --make-bed --out baseline_overlap --noweb --allow-no-sex

    # remove SNPs in different alleleic location
    plink --bfile ../../genepool_overlap --exclude genepool_overlap_missnp --make-bed --out genepool_overlap_qcplink --bfile reference_reich_qc --exclude reference_reich_qc_missnp --make-bed --out reich_here_qc2

    ### To avoid error in ADMIXTURE due to some of samples having all SNPs missing, do quality control for the set
    # Calculate missing rate 
    plink --bfile baseline_overlap --missing --out baseline_overlap --noweb

    # Get the number of SNPs in baseline_overlap
    wc -l baseline_overlap.bim # to get the number of SNPs in baseline_overlap

    # Get samples having all SNPs missing
    cat baseline_overlap.imiss  | awk '{if($4==109627) print $2}' >  baseline_overlap_missing_all_SNPs 

    # Remove collected samples
    cat baseline_overlap.fam | grep -wEf baseline_overlap_missing_all_SNPs > baseline_overlap_removeIndividual.txt  
    plink --bfile baseline_overlap --remove baseline_overlap_removeIndividual.txt --noweb --allow-no-sex --make-bed --out baseline_overlap_qc
  
    # Generate population  file for ADMIXTURE in supervised mode
    cut -f1-2 -d ' ' baseline_overlap_qc.fam > baseline_overlap_qc.pop.txt
    printf '%.0s\n' {1..1756}  > baseline_overlap_qc.pop
    cat baseline_overlap_qc.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> baseline_overlap_qc.pop

    # Run ADMIXTURE in supervised mode
    ../admixture32 baseline_overlap_qc.bed -F 9 -j8

    # Add header to Q file generated from ADMIXTURE
    cat baseline_overlap_qc.fam | cut -d ' ' -f1-2 > training_out_ind_id
    sed -i 's/ /\t/g' training_out_ind_id
    sed -i 's/ /\t/g' baseline_overlap.9.Q
    paste training_out_ind_id baseline_overlap.9.Q > out_Q_training_baseline_<num>
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_training_baseline  
  ```
  
  + 2.2.3. Model training for training set with non-curated SNPs (baseline)
    
  Baseline is used as a reference to see the performance of model with selected AIMs
  ```r
    qfile <- read.table('out_Q_training_baseline', header = T, sep = '\t')

    # Add meta information
    meta <- read.csv('../meta_table') 
  
    qfile_nogp <- qfile[-which(qfile$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                   'SouthAfrican', 'SouthWestAsian',
                                                   'NativeAmerican', 'Oceanian',
                                                   'SouthEastAsian', 'NorthernEuropean',
                                                   'SubsaharanAfrican')), ] 
    qfile_nogp$Populations<- as.character(qfile_nogp$Populations)
    qfile_nogp_popFilter <- add_meta_reich(qfile_nogp, meta)
    qfile_nogp_popFilter <- droplevels(qfile_nogp_popFilter)
    str(qfile_nogp_popFilter)
    save(qfile_nogp_popFilter, file = 'baseline_qfile.rdata')

    qfile_nogp_popFilter$GRC <- as.character(qfile_nogp_popFilter$GRC)
    if(sum(is.na(qfile_nogp_popFilter$longitude)) > 0){ # avoid the case that any longitude value from meat_table is invalid
      qfile_nogp_popFilter <- qfile_nogp_popFilter[-which(is.na(qfile_nogp_popFilter$longitude)),]
    }
    
    # Model training
    rf_model_training(qfile_nogp_popFilter,tag = ind)
  ```

  + 2.2.4. Make minor allele frequency (MAF) table for each sample 
                              
  Prepare frequency file and PED file for MAF table construction
  ```r
    system("plink --bfile baseline_overlap_qc --recode --tab --out CONVERTReference")
    system("plink --bfile baseline_overlap_qc --freq --noweb")
    system("mv plink.frq reference.frq")

    # If the disk size is highly limited, these files can be removed
    # system('rm baseline_overlap.*')
    # system('rm genepool_overlap*')
    # system('rm reference_reich*')
    # system('rm reich_here*')
    # system('rm CONVERTReference.map')
    # system('rm CONVERTReference.log')
    # system('rm CONVERTReference.nosex')

  ```
                              
  Construct MAF table
  ```r
    # extracted markers from reference
    marker_frq <- read.table('reference.frq')
    marker_ped <- read.table('CONVERTReference.ped', sep = '\t')
    
    metasub_data <- maf_tbl_generation(marker_frq, marker_ped,
                                       'MAF_reference.rdata')
    system('rm CONVERTReference.ped')

  ```
  
  Add meta data to MAF table 
  ```r
  load('MAF_reference.rdata')
  reference_fam <- read.table('reference_sample', sep='\t')[-1,]
  metasub_data <- metasub_data[-which(metasub_data$Populations %in% c('NorthEastAsian', 'Mediterranean',
                                                                      'SouthAfrican', 'SouthWestAsian',
                                                                      'NativeAmerican', 'Oceanian',
                                                                      'SouthEastAsian', 'NorthernEuropean',
                                                                      'SubsaharanAfrican')), ]
  
  print(paste0('nrow of metasub_data after removing gene pools: ',nrow(metasub_data)))
  
  metasub_data$GRC <- NA
  for(i in 1:nrow(metasub_data)){
    metasub_data$GRC[i] <- as.character(reference_fam$V2[which(reference_fam$V1 == metasub_data$Populations[i])])
  }
  print('metasub_data:')
  str(metasub_data)
  
  # Add meta information
  meta <- read.csv('../meta_table') 
  metasub_data_meta <- add_meta_reich(metasub_data, meta)
  
  if(sum(is.na(metasub_data_meta$longitude)) > 0){
    str(metasub_data_meta)
    stop(paste0('Number of NA longitude: ',sum(is.na(metasub_data_meta$longitude))))
  }else{
    metasub_data_meta$latitude <- as.numeric(metasub_data_meta$latitude)
    metasub_data_meta$longitude <- as.numeric(metasub_data_meta$longitude)
    
    print('metasub_data_meta:')
    str(metasub_data_meta)
    
    save(metasub_data_meta, file='metasub_data_maf_ref.rdata')
    system('rm MAF_reference.rdata')
  }

  ```
  
  feature selection with null importance
  ```r
  ##### Data preparation #####
  load('metasub_data_maf_ref.rdata')

  trial_dt <- metasub_data_meta
  trial_dt <- trial_dt[,c(2:(ncol(trial_dt)-4),
                          grep('latitude', colnames(trial_dt)),
                          grep('longitude', colnames(trial_dt)))] # only remains MAF columns, latitude column, and longitude column 

  x <- trial_dt
  features <- colnames(trial_dt)[c(1:(ncol(trial_dt)-2))]
  if(length(features) + 5 != ncol(metasub_data_meta)){ # 5 columns : Populations, GRC, country, latitude, longitude
    stop('Number of length +5 not equal to MAF table') # if the equation does not meet, more/less features were extracted
  }
  
  
  ##### benchmarking generation #####
  benchmark <- null_importance_select(x=x, features= features,
                                      shuffle=F, seed=123)
  print('benchmark:')
  head(benchmark)
  
  save(benchmark, file = 'benchmark.rdata')
  
  ##### null importance #####
  null_imp_df <- data.frame()
  nb_runs <- 80 # shuffle 80 times
  start_time <- Sys.time()
  dsp <- c() 
  for(i in 1:nb_runs){
    # Get current run importances
    imp_df <- null_importance_select(x=x, features= features,
                                     shuffle=T, seed=123)
    imp_df$run <- i
    # Concat the latest importances with the old ones
    if(nrow(null_imp_df)==0){
      null_imp_df <- imp_df
    }else{
      null_imp_df<- rbind(null_imp_df, imp_df)
    }
    # Display current run and time used
    spent = Sys.time() - start_time
    dsp <- c(dsp,paste0('Done with    ', i,' of ',nb_runs, '    (spent ', spent,')' )) 
    print(dsp)
    
  }
  end_time <- Sys.time()
  time_for_testPreds_ind <- end_time - start_time
  print(time_for_testPreds_ind)
  
  print('null_imp_df:')
  head(null_imp_df)
  
  save(null_imp_df, file = 'null_imp_df.rdata')

  ##### sync features in null importance and benchmark #####
  load('null_imp_df.rdata')
  load('benchmark.rdata')
  null_imp_df.cp <- null_imp_df
  benchmark.cp <- benchmark
  null_imp_fea <- unique(null_imp_df.cp$feature)
  
  # if features in benchmark does not present in features in data frame having 80 shuffle feature importance results, add this benchmark feature.
  for(j in 1:nrow(benchmark.cp)){ 
    current_fea_benchmark <- benchmark.cp$feature[j]
    if(!current_fea_benchmark %in% null_imp_df$feature){
      null_imp_df <- rbind(as.data.frame(null_imp_df),
                           c(current_fea_benchmark, 0, 0, 0))
    }
  }
  if(sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea) != length(unique(null_imp_df$feature))){
    print(paste0('num of features in benchmark not in null_imp + num of features in null_imp: ',sum(!benchmark.cp$feature %in% null_imp_fea) + length(null_imp_fea)))
    print(paste0('num of features in null_imp after adding', length(unique(null_imp_df$feature))))
    stop('check the equivalence between benchmark$feature and null_imp$feature')
    
  }else{
    # if features in data frame having 80 shuffle feature importance results does not present in features in benchmark, add this feature
    for(i in 1:length(null_imp_fea)){
      current_fea <- null_imp_fea[i]
      if(!current_fea %in% benchmark$feature){
        benchmark <- rbind(as.data.frame(benchmark), c(current_fea, 0, 0))
      }
    }
    
    if(  nrow(benchmark) == length(unique(null_imp_df$feature)) ){
      benchmark$importance_gain <- as.numeric(benchmark$importance_gain)
      benchmark$importance_split <- as.numeric(benchmark$importance_split)
      null_imp_df$importance_gain <- as.numeric(null_imp_df$importance_gain)
      null_imp_df$importance_split <- as.numeric(null_imp_df$importance_split)
      null_imp_df$run <- as.numeric(null_imp_df$run)
      
      save(benchmark, file = 'benchmark_full.rdata')
      save(null_imp_df, file = 'null_imp_df_full.rdata')
      
    }else{
      print('num of features in benchmark: ', nrow(benchmark))
      print('num of features in null_imp: ', length(unique(null_imp_df$feature)))
      stop('num of row in benchmark does not equal to features in null_imp_df')
    }
    
  }
  
  
    ##### calculate the feature scoring for each feature in benchmark #####
  load('benchmark_full.rdata')
  load('null_imp_df_full.rdata')
  
  feature_scores <- data.frame(feature=NA, split_score=NA, gain_score=NA)
  for (f in unique(benchmark$feature)){
    f_null_imps_gain <- null_imp_df$importance_gain[null_imp_df$feature == f]
    f_act_imps_gain <- mean(benchmark$importance_gain[benchmark$feature == f])
    gain_score <- log(1e-10 + f_act_imps_gain / (1 + quantile(f_null_imps_gain, c(.75))[[1]]))  # Avoid divide by zero
    f_null_imps_split <- null_imp_df$importance_split[null_imp_df$feature == f]
    f_act_imps_split <- mean(benchmark$importance_split[benchmark$feature == f])
    split_score <- log(1e-10 + f_act_imps_split / (1 +  quantile(f_null_imps_split, c(.75))[[1]]))  # Avoid divide by zero
    feature_scores<- rbind(feature_scores,
                           c(f, split_score, gain_score))
    
  }
  
  scores_df <- feature_scores[-1,]
  scores_df$split_score <- as.numeric(scores_df$split_score)
  scores_df$gain_score <- as.numeric(scores_df$gain_score)
  
  print('75 percentile scores_df:')
  str(scores_df)
  
  save(scores_df, file = 'scores_df_75.rdata')
  
  #### select the features in top 300 feature score ####
    filter_selected_fea <- scores_df$feature[which(scores_df$gain_score>0 & scores_df$split_score>0)]

    filter_selected_fea.order <- filter_selected_fea[order(filter_selected_fea, decreasing = T),]
    write.table(filter_selected_fea.order[c(1:300)], file = 'split300.snp', quote = F, row.names = F, col.names = F)

  ```
                                 
  + 2.2.5. Extract selected SNPs in training set + Run ADMIXTURE in supervised mode for training set after feature selection
  
  ```r
     system("sh ../aft_fea_sel_command")
     system("rm selected_fea_score*")
  ```
                 
  where *aft_fea_sel_command*:
  ```bash session
     cat selected_fea_score.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> selected_fea_score.pop

      # Run ADMIXTURE in supervised mode
      ~/admixture32 selected_fea_score.bed -F 9 -j8

      # Add header to Q file generated from ADMIXTURE
      cat selected_fea_score.fam | cut -d ' ' -f1-2 > out_ind_id_selected_fea_score
      sed -i 's/ /\t/g' out_ind_id_selected_fea_score
      sed -i 's/ /\t/g' selected_fea_score.9.Q
      paste out_ind_id_selected_fea_score selected_fea_score.9.Q > out_Q_values_ref_selected_fea_score_$2
      sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African' out_Q_values_ref_split300
  ```
                                 
                           
 
  
                


