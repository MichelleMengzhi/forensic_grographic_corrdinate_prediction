# master degree project: a forensic grographic corrdinate prediction pipeline
#### course: degree project
#### credit: 45
#### student: Yuexin Yu
#### Supervisor: Eran Elhaik

This is the README file containing the workflow to construct this forensic geopragic cooridnate prediction pipeline.

The required input are available in *Data* directory.

All required packages in R are listed in packages.R

> Introduction. 

  This is an admixture-based pipeline to predict smaplesâ€˜ geographic coordinates in given test set. The pipeline applies the following general workflow:  
  * 1. Extract an AIM set from both training set and test set
  * 2. Calculate portions of pupative ancestral populations of traing set and test set repsectively, by ADMIXTURE in supervised mode.
  * 3. Predict the geographic coordinates for samples in test set based on the model trained by Random Forest using training set.   

> Load packages in R 4.1.2
``` r

```

  
> 0. Functions
```r
########## rf_latlong ##########
rf_latlong <- function(training, testing, variables, coast=NULL, nthread=8){
  # Model training by random forest method and use the model to predict test set. Then adjust predicted geographic coordinates based on world point data and coastline data
  # @training: the data frame of training set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @test: the data frame of test set, having sample names, corresponding ancestral population portion calculation, GRC, origin latitude and longitude, as well as country.
  # @variables: a list of column names used in model training
  # @coast: If coast = NULL, which is the feault, it means no need data adjustment after prediction; otherwise, it should be the coastline data generated in rf_model_training function. 
  # @nthread: number of threads to use when running this function. The default is 8.
  
  set.seed(1234)
  
  # create 5 subsets in training for cross validation
  folds <- caret::createFolds(training[,'Populations'], k = 5, returnTrain = T)
  
  # ask cross validation when model training; the model training will be a regression
  trControl <-  trainControl( # regression
    method = "cv",
    number = 5,
    verboseIter = FALSE,
    returnData = FALSE,
    search = "grid",
    savePredictions = "final",
    allowParallel = T,
    index = folds)
  
  # use a range of value as the value of parameter so that all value will be tested and the model with smallest RMSE will be finally used
  tune_grid <- expand.grid(.mtry = (1:15))
  
  # train for latitude
  training$rowIndex <- as.numeric(rownames(training))
  Xgb_latitude <- train(x = training[,variables],
                        y = training[,'latitude'],
                        method = "rf",
                        trControl = trControl,
                        tuneGrid = tune_grid ,
                        nthread = nthread
  )

  # train for longitude
  Xgb_longitude <- train(x = training[,variables],
                         y = training[,'longitude'],
                         method = "rf",
                         trControl = trControl,
                         tuneGrid = tune_grid ,
                         nthread = nthread
  )
  
  message(paste('the mean R2 for current model',
              mean(Xgb_longitude$results$Rsquared[which(Xgb_longitude$results$mtry == Xgb_longitude$bestTune[[1]])],
               Xgb_latitude$results$Rsquared[which(Xgb_latitude$results$mtry == Xgb_latitude$bestTune[[1]])])), 
          )
  
  
  latPred <- predict(Xgb_latitude, newdata = testing[,variables])
  longPred <- predict(Xgb_longitude, newdata = testing[,variables])
  
  #adjust out of bounds predictions
  message('adjust out of bounds predictions')
  longPred[longPred > 180] <- 180
  longPred[longPred < -180] <- -180
  latPred[latPred > 90] <- 90
  latPred[latPred < -90] <- -90
  #Pull to nearest coastline if provided
  # message('Pull to nearest coastline if provided')
  find_coast <- function(long, lat) { # find the closet point on the coast for the given long and lat
    distances_from_coastline <-
      sp::spDistsN1(coast, c(long, lat), longlat = TRUE)
    
    closest_point <-  which.min(distances_from_coastline)
    new_coords <- coast[closest_point,]
    
    return(new_coords)
  }
  
  if (!is.null(coast)) {
    message('toAdjust generated by function map.where')
    #find the longPred / latPred that are not on world latitude/longitude
    toAdjust <- which(is.na(maps::map.where(database = "world", longPred, latPred)))
    
    if(length(toAdjust) > 0){
      # apply find_coast function to adjust the latitude and longitude of given toAdjust index
      message('adjusted generated by mapply find_coast and longPred, latPred')
      adjusted <- mapply(find_coast, long = longPred[toAdjust], lat = latPred[toAdjust])
      
      # update the adjusted lat and long
      longPred[toAdjust] <- adjusted[1,]
      latPred[toAdjust] <- adjusted[2,]
    }
    
    
    
  }
  
  
  message('return')
  return(list(latPred, longPred))
  
  
}





########## rf_model_training ##########
rf_model_training <- function(qfile_nogp_popFilter, tag, extraColumn=''){
  
  # @qfile_nogp_popFilter: the data frame with sample names, corresponding ancestral population portion calculations, sample's latitude and longitude, GRC, as well as country
  # @tag: a string which is the suffix added in the file path 
  # @extraColumn: a string. If run model training in random forest method, extraColumn = '', which is the default. If run model training in random forest with an extra column added, extraColumn = <the extra column name>
  
  # prepare coastline data for the adjustment of predicted geographic coordinate
  coastlines <- cbind("x"  = maps::SpatialLines2map(rworldmap::coastsCoarse)$x ,"y" =maps::SpatialLines2map(rworldmap::coastsCoarse)$y)
  coastlines <- coastlines[complete.cases(coastlines),]
  coastlines <- coastlines[coastlines[,1] < 180 ,]
  
  GeoPreds <- list()
  
  # extract the ancestral population names
  gp <- colnames(qfile_nogp_popFilter)[-c(1,2,12,13,14)]
  
  # add the name of extra column if needed
  if(extraColumn != ''){
    gp <- c(gp, extraColumn)
    
  }
  
  set.seed(18)
  
  # split qfile_nogp_popFilter into 5 subsets.
  trainFolds <-  caret::createFolds(qfile_nogp_popFilter$Populations, k = 5, returnTrain = T)
  
  GeoPreds <- list()
  
  # For each iteration, use 4 subsets as training set, the left one as test set to do model training and prediction. Since there are 5 subsets, iteration is 5 so that all samples are finally tested
  start_time <- Sys.time()
  for (i in 1:5){
    
    train <- qfile_nogp_popFilter[trainFolds[[i]],]
    test <- qfile_nogp_popFilter[-trainFolds[[i]],]
    
    start_time.adm <- Sys.time()
    testPreds <-rf_latlong(training = train, testing = test,
                           variables = gp, coast=coastlines)
    GeoPreds[[i]] <- testPreds
    
    end_time.adm <- Sys.time()
    time_for_testPreds <- end_time.adm - start_time.adm
    # message(time_for_testPreds)
    
    
  }
  end_time <- Sys.time()
  time_for_testPreds_ind <- end_time - start_time
  message(time_for_testPreds_ind) # 
  
  # Add predicted geographic coordinates to corresponding samples
  add_preds <- list()
  for (i in 1:5){
    
    add_preds[[i]] <- cbind(qfile_nogp_popFilter[-trainFolds[[i]],] ,
                            "latPred" = GeoPreds[[i]][[1]],
                            "longPred" = GeoPreds[[i]][[2]] )
    
  }
  
  # Add predicted geographic coordinates to corresponding samples
  MetasubDataPreds <- plyr::rbind.fill(add_preds)
  
  # Save the data frame as a csv file
  if(extraColumn == ''){
    write.csv(MetasubDataPreds,paste0("qfile_predict_rf_",tag,".csv"))
    
  }else{
    write.csv(MetasubDataPreds,paste0("qfile_predict_extraCol_",tag,".csv"))
    
  }
  
  # MetasubDataPreds <- read.csv(paste0("qfile_predict_rf_",tag,".csv"))
  
  # Calculate the distance between the original and the predicted geographic coordinates, added to column 'Distance_from_origin'
  for (i in 1:nrow(MetasubDataPreds)){
    MetasubDataPreds[i,"Distance_from_origin"] <- geosphere::distm(c(MetasubDataPreds[i,"longPred"],MetasubDataPreds[i,"latPred"]), c(MetasubDataPreds[i,"longitude"],MetasubDataPreds[i,"latitude"]), fun = geosphere::distHaversine)/1000
  }
  
  # Print distance from origin results 
  print(c(mean(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin ),
          median(MetasubDataPreds$Distance_from_origin[which(MetasubDataPreds$Distance_from_origin < 100)])))
}




```
> 1. AADR set data preparation
  Codes here are perfomed in Bash command. [PLINK 1.9](https://www.cog-genomics.org/plink/1.9/) is required. Since some of file sizes here are over the size limit on github, only the result files with the prefix *reich_here_overlap* are provided
  ```console
  # Get overlaps between ancestrial population set and AADR set
  plink --bfile ../Genographic/num_Admixture_reference_pops --extract reich_here.bim --make-bed --out genepool_overlap_SNP --noweb
  
  # keep only overlapped SNPs in AADR set
  plink --bfile reich_here --extract ../Genographic/num_Admixture_reference_pops.bim --make-bed --out reich_here_overlap --noweb
  ```
  
> 2. AIM set curation using AADR set 

Note: Only the final result files (in 2..) is provided in *Data* directory

* 2.1. Randomly split AADR samples into 2 based on a filtration criteria, do 100 times

The filtration criteria:  

Before data splitting, countires having sample size < 5 should be discarded.  

During splitting, split smaples in each country into 2 sets in same size. If the sample size after splitting < 5, all samples in this country are put into training set. Otherwise, training set and test set will get random samples from this country in same size.   

Codes here are performed in R 4.1.2. 

```r
  # load meta
  meta <- read.csv('Data/meta_table') #nrow(meta)=14008
  # load sample tbl
  fam <- read.table('Data/reich_here_overlap.fam')[,2] 
  fam_file <- read.table('Data/reich_here_overlap.fam')
  meta <- meta[which(meta$Version.ID %in% fam),] 

  # remove countries having samples < 5
  ctry_count <- as.data.frame(table(meta$Country))
  smaller_than_five <- ctry_count$Var1[which(ctry_count$Freq<=5)]
  meta <- meta[-which(meta$Country %in% smaller_than_five ),] 
  ctry_count <- ctry_count[-which(ctry_count$Freq<=5),] 


  for(j in 1:100){
  sample_set1 <- c()
  sample_set2 <- c()
  
  # select half of samples from each country
   for(i in 1:nrow(ctry_count)){
     ctry <- ctry_count$Var1[i]
     ctry_sample <- meta$Version.ID[which(meta$Country == ctry)]
     split_size <- ceiling(length(ctry_sample)/2)
     if(split_size >= 5){ # if sample size after splitting < 5, all samples in this country are put into training set
       sample_set1 <- c(sample_set1, sample(ctry_sample, size = split_size))
       sample_set2 <- c(sample_set2, ctry_sample[-which(ctry_sample %in% sample_set1)])
     }else{ # if not, put into 2 sets
       sample_set1 <- c(sample_set1, ctry_sample)
     }
   }
   if(length(sample_set1) + length(sample_set2) == nrow(meta)){ # ensure no sample missing
     reference_sample <- fam_file[which(fam_file$V2 %in% sample_set1),]
     test_sample <- fam_file[which(fam_file$V2 %in% sample_set2),]
      
     # save them into file, ans save to a corresponding directory
     dir.create(paste0('dt',j))
     write.table(reference_sample, file = paste0('dt',j,'/reference_sample'),
                  quote = F, row.names = F, sep = '\t')
     write.table(test_sample, file = paste0('dt',j,'/test_sample'),
                  quote = F, row.names = F, sep = '\t')
   }else{
     message(nrow(reference_sample))
     message(nrow(test_sample))
     stop(paste0('In iteration ',j,', the sum of reference sample size and test sample size is not equal to the size of Reich dataset'))
   }
  
  
  }

```

* 2.2. For each set of split AADR training and test sets:
  
All R code in section 2.2. can be in one R file *ref_pipeline.R*, and run with arugument passing through this script in Bash command such as:

(Note that *<num>* represents *the number of current set from 100 runs*)

```console
  Rscript --vanilla ref_pipeline.R <num>
```
  
In ref_pipeline.R, start with:
```r
  args <- commandArgs(trailingOnly=TRUE)
  ii <- args[1]
  setwd(paste0('dt',ii))
```
  
to navigate to the directory having split sample sets in the number of current set from 100 runs.

  + 2.2.1. Extract samples for training set and test set
  
  Codes here are performed in R. PLINK 1.9 is required. 
  ```r
     system('plink --bfile ../../reich_here_overlap --keep reference_sample --make-bed --out reference_reich --noweb')
     system('plink --bfile ../../reich_here_overlap --keep test_sample --make-bed --out test_reich --noweb')

  ```
  + 2.2.2. Prepare training set + run ADMIXTURE in supervised mode for training set
  
  Codes here are performed in R. Both [PLINK 1.07](https://zzz.bwh.harvard.edu/plink/download.shtml) and PLINK 1.9 are required.
  ```r
    system('sh baseline_preparation')
  ```
  
  where *baseline_preparation*:
  
  (Note that ADMIXTURE is required, and is prepared in main page named as *admixture32*)
  ```console
    # since the coding of base is different for AADR set and ancestral population set, convert the base in AADR set
    ~/bin/plink-1.07-x86_64/plink --bfile reference_reich --allele1234 --make-bed --out reference_reich_qc --noweb

    # try to merge training set with ancestral population set, will get an error due to different allelic location in 2 sets, will automatically generate a .missno file
    plink --bfile reference_reich_qc --bmerge ../../genepool_overlap.bed ../../genepool_overlap.bim ../../genepool_overlap.fam  --make-bed --out baseline_overlap --noweb --allow-no-sex

    # remove SNPs in different alleleic location
    plink --bfile ../../genepool_overlap --exclude genepool_overlap_missnp --make-bed --out genepool_overlap_qcplink --bfile reference_reich_qc --exclude reference_reich_qc_missnp --make-bed --out reich_here_qc2

    ### To avoid error in ADMIXTURE due to some of samples having all SNPs missing, do quality control for the set
    # Calculate missing rate 
    plink --bfile baseline_overlap --missing --out baseline_overlap --noweb

    # Get the number of SNPs in baseline_overlap
    wc -l baseline_overlap.bim # to get the number of SNPs in baseline_overlap

    # Get samples having all SNPs missing
    cat baseline_overlap.imiss  | awk '{if($4==109627) print $2}' >  baseline_overlap_missing_all_SNPs 

    # Remove collected samples
    cat baseline_overlap.fam | grep -wEf baseline_overlap_missing_all_SNPs > baseline_overlap_removeIndividual.txt  
    plink --bfile baseline_overlap --remove baseline_overlap_removeIndividual.txt --noweb --allow-no-sex --make-bed --out baseline_overlap_qc

    # Generate population  file for ADMIXTURE in supervised mode
    cut -f1-2 -d ' ' baseline_overlap_qc.fam > baseline_overlap_qc.pop.txt
    printf '%.0s\n' {1..1756}  > baseline_overlap_qc.pop
    cat baseline_overlap_qc.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> baseline_overlap_qc.pop
  
    # Generate population  file for ADMIXTURE in supervised mode
    cut -f1-2 -d ' ' baseline_overlap_qc.fam > baseline_overlap_qc.pop.txt
    printf '%.0s\n' {1..1756}  > baseline_overlap_qc.pop
    cat baseline_overlap_qc.pop.txt | grep -E 'NorthEastAsian|Mediterranean|SouthAfrican|SouthWestAsian|NativeAmerican|Oceanian|SouthEastAsian|NorthernEuropean|SubsaharanAfrican' | cut -f1 -d' ' >> baseline_overlap_qc.pop

    # Run ADMIXTURE in supervised mode
    ../admixture32 baseline_overlap_qc.bed -F 9 -j8

    # Add header to Q file generated from ADMIXTURE
    cat baseline_overlap_qc.fam | cut -d ' ' -f1-2 > training_out_ind_id
    sed -i 's/ /\t/g' training_out_ind_id
    sed -i 's/ /\t/g' baseline_overlap.9.Q
    paste training_out_ind_id baseline_overlap.9.Q > out_Q_training_baseline_<num>
    sed -i '1 i\Populations\tGRC\tMediterranean\tNative American\tNortheast Asian\tNorthern European\tOceanian\tSouthern African\tSoutheast Asian\tSouthwest Asian\tSubsaharan African'  out_Q_training_baseline  
  ```
  
  + 2.2.3. Model training for training set with non-curated SNPs (baseline)
    
  Baseline is used as a reference to see the performance of model with selected AIMs
  ```r
    qfile <- read.table('out_Q_training_baseline', header = T, sep = '\t')

    # Add meta information
    meta <- read.csv('../meta_table') 
    source('../add_meta.R')

    qfile_nogp <- qfile[-which(qfile$Population %in% c('NorthEastAsian', 'Mediterranean',
                                                   'SouthAfrican', 'SouthWestAsian',
                                                   'NativeAmerican', 'Oceanian',
                                                   'SouthEastAsian', 'NorthernEuropean',
                                                   'SubsaharanAfrican')), ] 
    qfile_nogp$Populations<- as.character(qfile_nogp$Populations)
    qfile_nogp_popFilter <- add_meta_reich(qfile_nogp, meta)
    qfile_nogp_popFilter <- droplevels(qfile_nogp_popFilter)
    str(qfile_nogp_popFilter)
    save(qfile_nogp_popFilter, file = 'baseline_qfile.rdata')

    qfile_nogp_popFilter$GRC <- as.character(qfile_nogp_popFilter$GRC)
    if(sum(is.na(qfile_nogp_popFilter$longitude)) > 0){ # avoid the case that any longitude value from meat_table is invalid
      qfile_nogp_popFilter <- qfile_nogp_popFilter[-which(is.na(qfile_nogp_popFilter$longitude)),]
    }
    
    # Model training
    rf_model_training(qfile_nogp_popFilter,tag = ind)
  ```

  + 2.2.4. Make minor allele frequency (MAF) table for each sample 
                              
  Prepare frequency file and PED file for MAF table construction
  ```r
    system("plink --bfile baseline_overlap_qc --recode --tab --out CONVERTReference")
    system("plink --bfile baseline_overlap_qc --freq --noweb")
    system("mv plink.frq reference.frq")

    # If the disk size is highly limited, these files can be removed
    # system('rm baseline_overlap.*')
    # system('rm genepool_overlap*')
    # system('rm reference_reich*')
    # system('rm reich_here*')
    # system('rm CONVERTReference.map')
    # system('rm CONVERTReference.log')
    # system('rm CONVERTReference.nosex')

  ```
                


